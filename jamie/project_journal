Jamie Davis <davisjam@vt.edu>
22 October 2015

I went into src/threadpool.c and added printfs to 
 work
  > this actually performs a work item
    calls w->work
 uv__work_done
  > invokes the 'done' callback on a completed work item
    calls w->done

 include/uv-threadpool.h defines the uv__work item, with
 the work and done fields defined as:

   void (*work)(struct uv__work *w);
   void (*done)(struct uv__work *w, int status);

FORWARD ORDER
1
stat hello 0x1897380
stat world 0x1897aa0
rename 0x1896a40

2
stat hello 0x19cb380
rename 0x19caa40
stat world 0x19cbaa0

3
stat hello 0x1539380
stat world 0x1539aa0
rename 0x1538a40

4
stat hello
rename
stat world

5
stat hello
stat world
rename

6
stat hello
stat world
rename

7
stat hello
stat world
rename

8
stat hello
stat world
rename

REVERSED ORDER
1
rename 0x13c8848
stat world 0x1663aa0
stat hello 0x1663380

2
rename 0x2873a40
stat world 0x2874aa0
stat hello 0x2874380

3
rename
stat world
stat hello

4
rename
stat world
stat hello

5
stat world
rename
stat hello

------------------

Conclusion:
  0. file_system.js is a useful tool to explore node+libuv interactions.

  1. The order in which callbacks enter the queue in uv__work_done
    depends on the order in which the underlying FS operations complete.

  2. Once they reach the queue, we can re-order them, as demonstrated.
    However it's the thread pool ordering that determines the order 
    in which they reach the queue.
    I haven't yet identified a way to determine which callback is which
    across runs (i.e. which one is 'stat' vs 'rename' in run X?).
    This would allow me to re-order function and callback invocations.
    
  3. To do that, I think I need to know how 'work' and 'done' get set
    for each `struct uv__work'.

  4. Just because a 'stat' command succeeded does not mean that the 
     stat'd file still exists at the time of the callback.

  5. Can re-order things all we want (both threadpool execution order 
      and global callback order) in src/threadpool.c, PROVIDED
      we can identify what we're working with.

  6. The libuv forum suggests that the threadpool is only used to
      emulate asynchronous-less synchronous operations like AIO.
      Where are asynchronous-native operations like sockets handled?

Open questions:
  1. Where are asynchronous-native operations like sockets handled?
  2. How does node.js submit operation+callback requests to libuv?
      >> $NODE/src/*wrap.cc ?
  3. Can we add user-defined data to libuv structs for label propagation?

---------------------------------

Jamie Davis <davisjam@vt.edu>
23 October 2015

Yesterday I discussed the open questions above with Dr. Lee.
#1: He suggested that finding this out would be valuable.
#2, #3: This may be irrelevant. If a fork (and-wait?) approach is used,
  the parent can launch a child to explore the path not taken.
  In such a setting, there's no need to know WHAT the work payload is,
  because the memory address itself can be used as the ID.
  I was thinking that we had to propagate labels in order to identify items
  between runs, but if I stick to a PARTICULAR run then I can use fork to
  reorder arbitrarily. 
  
  See test_code/fork_test.c and test_code/fork_test.txt for some sample code 
    along these lines.

  New open questions:
  1. Where are asynchronous-native operations like sockets handled?
     Is every callback executed in uv__work_done?
      > Explore http server, since socket code is not done by threadpool.
        I believe the answer is yes.
  2. Play with fork!

-------------------------

24 October 2015

Addressing the open questions from 23 October:

1. Regarding asynchronous-native operations like sockets:
   The callback is executed in uv__io_poll.
   Testing with 'node simple_http':
     uv__io_poll: calling w 0x2bd84a8's cb 0xdbff40 with {events 1, data 13}
     APP: Server handled a client!
     uv__io_poll: finished calling w 0x2bd84a8's cb 0xdbff40 with {events 1, data 18446744073709551615}

   The callback function is:
     gdb `which node`
     info symbol 0xdbff40
     uv.stream_io in section .text

     src/unix/stream.c:

   Uncomfortably, multiple callbacks can be pending in each phase of the loop, and furthermore
     in uv__io_poll we may call a single callback that routes us into threadpool: uv__work_done
     which calls multiple callbacks.
     It's not clear how to change the order of these callbacks at the level of uv__io_poll.

   Suppose that for starters we only re-order callbacks in threadpool::worker and threadpool::uv__work_done.
   This should be enough to see some interesting behavior.

2. Playing with fork: 
   I modified uv__work_done as follows:
    - wait until 4 callbacks show up (not sure what they all are)
    - shmem
    - fork()
      > child goes first and runs callbacks in forward order
      > parent sees child is done, goes second, and runs callbacks in reverse order
    Result: success, callbacks are executed in forward and reverse order as expected.

   This raises new questions.

    1. Unlike race conditions in memory, in node the race conditions can and do modify external
       resources like the file system or databases. 

       Different copies must not be allowed to see each other's actions. This suggests the need
       to ``reset'' things to the previous state. In the filesystem, taking a snapshot/rsync-backup
       should suffice (provided it preserves things like atime "just in case"). The database presumably
       also live in the filesystem. Network traffic can't be rolled back, though, so if network traffic
       is involved then the recipient must be able to "roll back" as well.

       This may be a limitation on the types of Node.js applications we can test. 
       If we treat Node.js applications as a "black box" then we need a way to reset the entire system.

    2. Need a way to decide when we have enough pending callbacks to fork and execute.
       Perhaps every time a new callback is received we can fork () and either start executing or NOT start executing.

    3. Review papers on reducing the exponential search space, e.g. partial order reduction.

    4. What happens if one process errors out? I'm guessing it closes stdout and stderr, discomfiting us. Not sure though.
       Need to introduce a fatal error into file_system.js under a particular interleaving.

-------------------------

26 October 2015

4. above: Regarding error'ing out: 
  If one process exits, the other's output is unaffected. It must dup the file descriptors. In retrospect, this makes sense.
  However, if one process relies on the other to log its completion, then we're in trouble.
  Need to wait for completion OR exit. I added a working implementation of this in threadpool.c.

-------------------------

27 October 2015

Discussion with Dr. Lee:
  - Bounded waiting: in time and in number of items in the queue
  - Flip order of only the first X items in the queue
  - Can't change order of JUST execution (cb->work) or JUST cleanup (cb->done),
      because execution maps to the 'request time' and cleanup maps to the 'done time'.
      Two work items could be done in order A, B, and finished in order B, A.
  - Partial order reduction: 
    > Need to read these papers (static [80s], dynamic [00s])
    > Only explore interleavings that actually affect each other
    > Issue here -- how to know if they affect each other in the "black box" approach to callbacks?
      Callbacks X and Y may not be directly related (surface level), but X may trigger callback Z that will affect Y.

  - Bounded reordering: added env variable REORDER_DISTANCE. If 0, do nothing. Otherwise use that reorder distance. Default is 0.
    > Haven't implemented the behavior yet.   
    > One paper used 'forward/backward' rather than spawning K! children. This sounds like a good move.
  - Wrote min_reordering.js that will exit(0) if reordering < X, otherwise exit(1).
    min_reordering.js uses dynamically-generated code so that I don't have to eyeball addresses for correctness;
    each incrementer function has its own "name". Hooray for closures!
  - Developed doubly linked list for a much-cleaner-to-work-with done_list.

-------------------------

28 October 2015

Implemented bounded waiting mechanism based on timeout and REORDER_DISTANCE.
Not quite working -- not quite understanding how to trigger epoll_wait to go back to uv__work_done.
  > output manipulation
  > log function that prefixes pid
  > figure out why epoll_wait isn't being nice

-------------------------

29 October 2015

Still working on the epoll_wait issue.
  - Added 'generation' to mylog for indentation. This should help clarify issues.
  - Idea: what if we spawn ALL children at a level before proceeding on one? This might help
      if the issue is some weird conflict between epoll and fork () (the internet suggests that
      there might be one).

      This leads to memory overhead (a weird combo of BFS and DFS search, but still painful), but 
      if it fixes the problem them that would be swell.

-------------------------

30 October 2015

- Used the combination DFS-BFS search as proposed on 29 Oct. This fixes the problem, hooray!

TODO (by order of priority):
  1. > Need a way to "prove" that it's working properly on a wide variety of inputs.
    Can generate simulated output for min_reordering.js and emit that, then compare.

  2. > Need to determine whether or not child exited 0.

  3. > Need to re-think the notion of a timeout. Because parents wait for children recursively,
    using clock time is a bad idea. 

    Propose instead using "number of loops since I last ran a callback". 
    This would require changing all callback calls to use a uv-common function that sets a 
    flag and then calls the callback. Then in main loop we could bump a counter, also through
    uv-common

  4. > Must avoid "wait forever"-style epoll requests. Never return -1 from the timout function.
    Instead, add a sleep in the main loop -- this effectively simulates "forever", although
    if done naively this would cause "idle" callbacks (and others?) to be executed many more 
    times than might be expected. Might need to take more care with this change.

    3,4: If each loop waits for a second, then "loops since I last ran a callback" has a lower bound of "seconds since I last ran a callback"

-------------------------

8 Nov 2015

- Fixed bug in list_size. Code is now in working order, ready to proceed on verifying correctness tomorrow.

-------------------------

9 Nov 2015

Addressing 1. from 30 October:
  1. Wrote simulator (min_reordering_exploration.pl) to simulate min_reordering.js's output
     when run under a modified Node. Initial results suggest that the simulator output matches
     the real output. Hooray!

  2. Implemented the node-mocks guy's fs.stat example to show the more general power of the 
     libuv reordering approach.

Addressing 2. from 30 October:
  1. Added exit status checking.

  2. It appears that fork() -> pthread_join() is a bad combination. See for example
      developerweb.net/viewtopic.php?id=3633
     Since this is a bit orthogonal, I just don't clean up the thread pool threads.

Outstanding TODOs, carried over from 30 October:
  1. Need to re-think the notion of a timeout. Because parents wait for children recursively,
    using clock time is a bad idea. 

    Propose instead using "number of loops since I last ran a callback". 
    This would require changing all callback calls to use a uv-common function that sets a 
    flag and then calls the callback. Then in main loop we could bump a counter, also through
    uv-common

  2. > Must avoid "wait forever"-style epoll requests. Never return -1 from the timout function.
    Instead, add a sleep in the main loop -- this effectively simulates "forever", although
    if done naively this would cause "idle" callbacks (and others?) to be executed many more 
    times than might be expected. Might need to take more care with this change.

    3,4: If each loop waits for a second, then "loops since I last ran a callback" has a lower bound of "seconds since I last ran a callback"

-------------------------

13 Nov 2015
    
Discussed progress with Dongyoon. Many ideas on paper.

-------------------------

16 Nov 2015

1. Question: How are fs.X and fs.XSync implemented?
  Answer: X is routed into libuv's thread pool work queue for asynchronous handling. XSync () is routed into libuv where it is called synchronously in uv__fs_work. The result is passed back up.

  See the '16 Nov 2015' entry in jamie_node_src_notes for details.

2. I reviewed issues with fork and pthreads. Most notably, in the child only the thread that
called fork() survives. I verified this using the fork_pthread.c test. 

Happily, Node.js only uses two sets of threads: the event loop thread (1 thread) and 
the thread pool threads (X threads).
As a result, getting to a coherent state prior to fork and re-initializing threads after fork
should not be difficult with the judicious use of locks.
  http://www.linuxprogrammingblog.com/threads-and-fork-think-twice-before-using-them
  http://pubs.opengroup.org/onlinepubs/009695399/functions/pthread_atfork.html

This probably also explains the segfault in pthread_join. Note that
both v8 and libuv make use of pthread_join, suggesting that we need
a pthread_at_fork call in both layers.

Question: When and why does v8's Thread::Start get called? I have added prints to show that it happens,
but have not yet tracked down WHY it happens. Next step: gdb

  cdprog
  gdb `which node`

  (gdb) break v8::base::Thread::Start
  Breakpoint 1 at 0xdfa174
  (gdb) c
  The program is not being run.
  (gdb) run file_system.js 
  Starting program: /home/jamie/bin/node file_system.js
  Traceback (most recent call last):
    File "/usr/share/gdb/auto-load/usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.19-gdb.py", line 63, in <module>
      from libstdcxx.v6.printers import register_libstdcxx_printers
  ImportError: No module named 'libstdcxx'
  [Thread debugging using libthread_db enabled]
  Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".

  Breakpoint 1, 0x0000000000dfa174 in v8::base::Thread::Start() ()
  (gdb) bt
  #0  0x0000000000dfa174 in v8::base::Thread::Start() ()
  #1  0x0000000000dcb9bc in v8::platform::WorkerThread::WorkerThread(v8::platform::TaskQueue*) ()
  #2  0x0000000000dc9191 in v8::platform::CreateDefaultPlatform(int) ()
  #3  0x0000000000d68e9e in node::Start(int, char**) ()
  #4  0x00007ffff6becec5 in __libc_start_main (main=0x688310 <main>, argc=2, argv=0x7fffffffe508, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffffffe4f8)
        at libc-start.c:287
  #5  0x000000000068852f in _start ()
  (gdb)

Answer: The threads are launched as part of default_platform, whatever that is.

Conclusion:
pthread_atfork needed to be added at each layer that calls pthreads.
Two places:
  1. V8 uses a pool of threads for its default_platform (whatever that is)
    At the V8 layer, these threads seem to be used to provide "isolates" (independent interpreters?), and it doesn't look like Node makes use of "isolates".
    As a result, no synchronization seemed to be required. I just added a Reinitialize function that would empty and re-create the thread pool.
    
  2. libuv uses a pool of threads for its asynchronous worker pool
    These worker threads are all active, and an uncontrolled fork() could result in inconsistent memory states,
      locked-never-to-be-released mutexes, etc.
    To resolve this, I added a posix semaphore to control the number of active threads. 
    By down'ing this semaphore nthreads times and waiting for all threads to be waiting, I can ensure that
      all worker threads are in a quiesced state at the time of fork ().
      On resume: 
        - the parent post's to the sema
        - the child re-initializes all of the pthread variables and launches nthreads threads

-------------------------

19 Nov 2015

1. Following up on the pthread question, I put printfs into the code to figure out the path by which
the v8 pthreads are assigned work. Work gets into their shared queue via DefaultPlatform::CallOnBackgroundThread

CallOnBackgroundThread seems to be used for compilation and garbage collection tasks
(grep'd the full tree including node and v8). It is called here:

v8/src/optimizing-compile-dispatcher.cc

  OptimizingCompileDispatcher::QueueForOptimization calls:
      V8::GetCurrentPlatform()->CallOnBackgroundThread(
              new CompileTask(isolate_), v8::Platform::kShortRunningTask);


  OptimizingCompileDispatcher::Unblock() calls:
      V8::GetCurrentPlatform()->CallOnBackgroundThread(
              new CompileTask(isolate_), v8::Platform::kShortRunningTask);
  
v8/src/heap/mark-compact.cc
  MarkCompactCollector::StartSweeperThreads calls:
    V8::GetCurrentPlatform()->CallOnBackgroundThread(
          new SweeperTask(heap(), heap()->old_space()),
                v8::Platform::kShortRunningTask);

The sample cluster.js program is sufficient to exercise these threads.
This means that I DEFINITELY need to correctly re-initialize them in a controlled fashion.

Since the WorkerThreads end up waiting on a semaphore in TaskQueue::GetNext, there's already
a built-in waiting mechanism. I added a counter to track the number of waiters, and taught
DefaultPlatform to wait for the queue to empty and all the waiters to be pending prior to
fork'ing. Since the tasks seem to be short (kShortRunningTask, whatever that means!), 
I expect that if the main thread pauses to wait for it to empty, then it will empty fairly quickly.

Implementation appears to have worked. I added a few functions to TaskQueue and now
DefaultPlatform asks it if all of its threads are waiting on it. Since no OTHER threads should be
waiting on the DefaultPlatform's TaskQueue (right?), this seems fine.

2. The Node.js cluster module uses fork(). This makes it unsuitable
to use in my testing environment, since I fork() too. The parent does not know what pid to wait on, and there's
no (?) way to tell the parent that it has a NEW child to wait on (and you'd need to fork the parent itself
to wait on the new child every time the child forks...). Anyway it just seems like a mess.

3. I read the microarchitecture paper on Node.js (I-cache performance issues). It reminded me that as a largely-server-side programming framework, we'll need to think about network inputs as we fork. If we run through once to completion, we can record the network inputs. Afterward we can simulate them? Hmm...

4. I took a peek at how callbacks are invoked:

jamie@lqingrui:~/Desktop/node_project/node/deps/uv/src$ grep -Ri cb\( . | wc -l
141

Threadpool callbacks take 1 or 2 args:
  jamie@lqingrui:~/Desktop/node_project/node/deps/uv/src$ grep -Ri cb\( . | grep threadpool
  ./threadpool.c:  req->work_cb(req);
  ./threadpool.c:  req->after_work_cb(req, err);


More general callbacks take between 1 and 5 args. Here are examples of each:
  ./unix/fs.c:  req->cb(req);
  ./unix/udp.c:      req->send_cb(req, req->status);
  ./unix/getaddrinfo.c:    req->cb(req, req->retcode, req->addrinfo);
  ./unix/getnameinfo.c:    req->getnameinfo_cb(req, req->retcode, host, service);
  ./win/udp.c:      handle->recv_cb(handle, UV_ENOBUFS, &buf, NULL, 0);

19 Nov 2015
Meeting with Dr. Lee.

1. He has a literature list, to which I am welcome to add. For the next meeting, read the Bounded POR and DPOR papers.
2. Discussion about dealing with network traffic. I assert that the major use case for Node.js is as a server to fulfill network requests.  Consequently it's important to be able to handle this.  Probably not an open system (maintaining connections tricky), but can make a closed system (feed the same input). This requires that the client's input does not change based on the server's output. There are clearly cases where this is not true (e.g. two concurrent requests, one to delete a file and one to stat it, and the output will be either "no such file" and "stat values" -- the client will respond differently based on the server's decisions. So let's not discard an open system forever, but for now it's probably necessary to get something working.

3. Proposed architecture:
A. Fork preserves memory state
B. Use file system checkpointing to roll back changes from one run to the next
C. Record network inputs (connections, client data, etc.) and feed that into subsequent runs. Can record the loop number so that we can feed it at the appropriate spot. Modify the epoll mechanism to replay network input at the appropriate spot instead of looking for a live connection. Children close the parent's connections.

4. Question: Node.js uses a lot of external modules. Are these pure JS? Do they interact with libuv? Do they run JS code in the thread pool "work" or just as "done" (e.g. database requests/callbacks)? (single-threaded vs. multi-threaded event driven)

5. Plan to attend ASPLOS Apr. 2-6 '16 in Atlanta.

-----------

15 Dec 2015

1.
I've implemented a unified callback mechanism in an attempt to avoid the issues related to fork.
jamie@suwarna7-Lenovo-K450e:~/Desktop/node_project/node/deps/uv$ git branch
  master
* unified_callback
  uv_reordering

I construct a tree of dependencies between callbacks based on the currently-executing CB.
I'm working on verifying the result. The graphviz library and the xdot viewer for it look useful.

-----------

16 Dec 2015

1. I added a bit more information to a 'struct callback_node'. Now on USR2, the trees are printed 
in graphviz's DOT language for easy visualization. Looks good.

2. The tree looks OK but there's no information about cause and effect. 
In the MUD Node.js project, when a client makes a move it triggers messages to the other clients.
However, the subsequent messages are not children of the client CB.
If I can figure out when the fds of the client CBs are triggered, I can include the info about "who tickled me"
and expand my "cause and effect" picture.

-----------

17 Dec 2015

1. I set the WORK_WORK CBN as the parent of the subsequent WORK_DONE CBN.
2. Now dumping all trees in one meta-tree on receipt of SIGUSR2.
3. Trying to analyze the generated CB tree to determine why there are
   so many WORK_* CBs. It looks like there are some internally-generated
   work items chained together to implement readFile. Perhaps it's 
   open-read-close or something. Presumably a library thing.
4. It seems that responding to a client causes Node (V8?) to generate a TIMER_CB. Not sure why. Presumably a library thing.

-----------

13 Jan 2016

1. Checked on the clients associated with the 6 benchmarks. Lowering the
start window to 5 seconds made this more friendly on MUD, though not sure
if this is going to be bad for others. URL is no good, Dr. Lee is working
on it.
2. Refreshed self on status of code base
3. On SIGINT, dump both visualizations (global callback order and tree view) and exit(0)
4. Unified callback: async calls in init stack
   
   Problem: If the application makes an async call in the initial
   stack (as done in the Micro '15 benchmarks code with an IO
   to an output file), then we didn't have a parent. We assumed async
   calls (to the threadpool ) would always have a parent.
   
   Solution: When submitting work to the threadpool, if there is no
   active CBN then we assume we're in the initial stack. Add a special
   function to return a dummy CBN in this case.
5. Now printing the full tree output to a file in /tmp rather than to stdout. This will allow chaining more readily.

-----------

14 Jan 2016

1. On MUD, verified that the number of callback trees is roughly a function of the number of clients. This is good, since it means that I'm not getting totally bogus-looking numbers.
2. Now printing global callback order to a file, along with client ID and start/duration (in seconds since epoch).
3. Discovered that using the fd associated with a handle as a "client id" will not work. In MUD, for example, handles are opened and closed for each http request -- so 5 requests from 8 clients yields 40 open-and-closed handles. The fds will be re-used. Instead we need to really know the client IP and port. Perhaps this can be stored in the "void* uv_handle_t.data" field of a handle when the handle is created by Node.js? Alternatively, more specific subclasses of a handle DO know this info. For example a uv_udp_t has uv_udp_getsockname() for this purpose. Some investigation required.

-----------

15 Jan 2016

1. Wrote a callback statistics calculator, callback_stats. The callback_stats utility calculates interesting statistics on the "all callbacks" file produced by libuv. This can be used to characterize the behavior of Node.js applications, or of a single application across different inputs.

Sample output is below. Note that it is "bogus" in the sense that client calculations are still incorrect (see NEXT STEPS).
However, the total count of callbacks and the per-type counts are accurate.

jamie@suwarna7-Lenovo-K450e:~/bin$ ./callback_stats /tmp/callback_global_order_1452867867_24003.txt
Results from /tmp/callback_global_order_1452867867_24003.txt
------------------------------------------------------------------------------------
Statistic                                                       Value               
------------------------------------------------------------------------------------
Instances of type UV_READ_CB                                    80                  
Client -1: Instances of type UV_ASYNC_CB                        3                   
Instances of type UV__WORK_WORK                                 3                   
Client 12: Instances of type UV_CONNECTION_CB                   40                  
Client 11: Instances of type UV_READ_CB                         80                  
Instances of type UV__IO_CB                                     206                 
Client -1: Instances of type UV_TIMER_CB                        1                   
Client -1: Instances of type UV__WORK_DONE                      3                   
Instances of type UV__WORK_DONE                                 3                   
Client -1: Instances of type UV_FS_CB                           3                   
Instances of type UV__ASYNC_CB                                  3                   
Client -1: Instances of type UV__WORK_WORK                      3                   
Callbacks with 0 children                                       327                 
Instances of type UV_CONNECTION_CB                              40                  
Client 12: number of callbacks                                  40                  
Instances of type UV_ALLOC_CB                                   80                  
Client 11: number of callbacks                                  160                 
Instances of type UV_TIMER_CB                                   1                   
Client -1: Instances of type UV_CLOSE_CB                        40                  
Instances of type UV_FS_CB                                      3                   
Client -1: Instances of type UV__IO_CB                          206                 
Client -1: Instances of type UV__ASYNC_CB                       3                   
Instances of type UV_CLOSE_CB                                   40                  
Client 11: Instances of type UV_ALLOC_CB                        80                  
Client -1: number of callbacks                                  262                 
Callbacks with 1 children                                       55                  
Callbacks with 2 children                                       80                  
Number of callbacks                                             462                 
Instances of type UV_ASYNC_CB                                   3                   
Number of clients                                               3     

-----------

18 Jan 2016

1. Made lists thread-safe internally.
2. Developed a map class for future use. It does not appear to be immediately relevant but "you never know". I thought it would be useful but then I came up with a better approach.
3. Determined that with a tcp handle I can identify the IP and (port? socket?) of the client. This information can be inherited by children. See notes from 1/18.

-----------

19 Jan 2016

1. Debug map class. Debug list class. Add unit tests for each. Commit changes.
2. Make an offline backup of progress so far.
3. Work on client_id. Experiment with lets-chat to see what another set of trees looks like.
  .dot files:
  ls /tmp/*clients.dot
    /tmp/lets-chat_clients.dot  /tmp/lets-chat_manual_clients.dot  /tmp/mud_clients.dot

-----------

20 Jan 2016

1. Implement two client ID maps. The first map is <internal client ID -> hash(sockaddr)>, and the second is <hash(sockaddr) -> "struct sockaddr">. In invoke_callback, if a stream is available then we can (unsafely!) obtain the struct sockaddr associated with it. From this we can determine the client ID. The client ID is based on a hash of the client's IP address and port number.

  On the utility of this approach: 
    mud: The http request embeds the client ID. Each move triggers a new http request (connection) that uses a new port, which causes each user's moves to manifest as a different client. This seems like an inefficient implementation to me. However, the point here is that the application can do "whatever it wants", and this may compromise attempts to identify clients at the framework level.
    lets-chat: There are some unknown client IDs going on here, but the implementation overall has all chat traffic from a user routed through the same connection. I have not tried clicking other buttons. For example, if I start a new chat room, does libuv still know it's me, or does it think I'm a new user?

  A general solution here would be to have one IP per client, and to reduce the client ID to a computation solely based on IP address. This seems like a workable approach, though at the moment it's not good because of the way clients work.
  Another approach would be application-level annotation. This would be undesirable.

2. In progress: once one node in a tree knows its client ID, the entire tree is marked as belonging to that client.

3. Enhance callback_stats to extract and print the "complete schedule" -- the order in which client-driven callbacks (i.e. callbacks that originated from a client).

-----------

21 Jan 2016

1. Think carefully about replayability and tree structures.
2. First draft of correct timer inheritance.

-----------

25 Jan 2016

1. Correct timer inheritance.
2. Color nodes based on client ID.
3. Improved detection of client ID.
4. Add new API uv_init_stack_done so that libuv can distinguish between CBs registered/executed
   in the initial stack vs. those triggered by client input.

-----------

26 Jan 2016

1. Debugging/committing of the 25 Jan. changes.
2. Fix misc compilation warnings.
3. Capture check, prepare, and idle callback registration and inheritance. See src/unix/loop-watcher.c
4. Microsecond granularity for a CBN's "start" field. Otherwise the field was useless.

-----------

27 Jan 2016

I proofread Dr. Lee's TxRace paper for ASPLOS '16 from noon until 11:30 PM.

-----------

28 Jan 2016

1. Mark begin and end of init stack and uv_run. 
Next step: Create a map of callback_address -> source,
where source is one of {NODE_INTERNAL, USER}. Instrumenting the 
callback registration points combined with calls to uv__init_stack_active
and uv__uv_run_active should suffice. 
I am assuming that only one Node implementation thread might interact with uv at a time. We'll see how true that is. If not true, the recourse would be
a per-thread status variable. Easy enough -- note the tid of all threads
that register callbacks.

-----------

29 Jan 2016

No work today.

-----------

1 Feb 2016

1. Complete implementation of origin tracking. This lets me identify CBNs originating from internal Node.js and ibuv code. I can now identify everything but leftover "UV__IO_CBs" that emerge during uv__run_pending. So far I have not seen these register any CBs, so I think this is OK. However, I regret not knowing which CB caused their registration. Perhaps this could be tracked later.
2. Refactor invoke_callback a bit to reduce bloat. Make use of helper functions.
3. Use clock_gettime for a monotonically increasing "start" value. Also use this for the duration calculations. 

-----------

2 Feb 2016

1. Refactor invoke_callback a bit more so that the giant switch is in its own function. Makes invoke_callback more readable.
2. Play on systemG.
3. bitbucket/noderacer: vanilla mode (no Jalangi).
4. Think about uv__run_pending lineage determination.

-----------

3 Feb 2016

1. Play with systemG. Get passwordless ssh, pdsh, bitbucket, etc.
2. Think about uv__run_pending lineage determination.
3. bitbucket/noderacer: vanilla mode (no Jalangi), pick your server IP and port

-----------

4 Feb 2016

1. Implement uv__run_pending lineage determination. Verify that things "look OK" on MUD and lets-chat.
2. Add an assert that threadpool threads only run "expected" CBs (UV__WORK_WORK and UV_WORK_CB).
3. Bugfix: Set init_stack_CBN as the current CBN during the initial stack so that all CBs descend appropriately during the initial stack. 
4a. Implement per-thread current CBN. Still need to propagate this change to affected portions of uv-common.c

-----------

5 Feb 2016

1. Re-work of lineage determination for threadpool. Realized I was doing this a bit incorrectly. Alas. Should explain the unknown+oddly-located WORK CBs in lets-chat. See notes in jamie_libuv_src_notes for details.
  Currently debugging my solution. Blowing up in lets-chat with a CB pointing to a CBN?? 


-----------

6-9 Feb 2016

1. Change tree design a bit. I decided that changing the parentage of a node was unwise at best. This resulted in an after-the-fact change to the schedule, which would make it impossible to "retrace my steps" on a subsequent run given a schedule. Instead, I'm now maintaining dual physical/logical parentage information. The physical tree is a faithful reproduction of the actual order in which callbacks passed through libuv during a run. The logical tree provides extra edges where relationships are not provided by the physical tree. For example, the relationship of repeating timers to themselves, or the relationships of UV__WORK_WORK to UV__WORK_DONE, is not visible from a physical tree. However, tracking it is important, and the logical tree gives us that. In essence, I'm now keeping ALL the information, instead of throwing some of it away.
TODO Discuss thoughts on inheritance and coloring

-----------

9 Feb 2016

1. Eliminate all tree inheritance and coloring. I believe that inheritance coloring should be
  added back in, and should cross logical barriers. However, I think tree coloring is unsafe.
  Consider for example the effect on httpserver_batch.js.

2. cdprog: Developed httpserver_{straightforward, batch, background} to demonstrate the callback tree
  associated with each. Use ./http_client.js to exercise them.

3. Change the shape of CB nodes based on whether they corresponded to client input, output, or neither. 
  Note: READ callbacks are invoked when data is read from a stream (client). These CBs will begin the response to the client's input. WRITE callbacks are invoked after data is written to a stream (client). These CBs come at the END of the output. This means that delaying the execution of a WRITE callback does not actually delay the output, just the notification of the output. If the output is coming indirectly (e.g. httpserver_batch), grabbing the WRITE callback won't tell us this.
  This suggests that marking the callback in uv_write2 as "output to client X" would be a useful step.

4. Write a program that uses async.parallel to see what it looks like at the libuv level

5. I've decided that determining which CBs are client code is probably not helpful (even if I could figure out how to do it -- via origin?). UV__WORK_WORK is not client code, but it includes the Node.js implementation of FS IO. Essentially UV__WORK_WORK *is* client code, but it's not registered as such. Rather, inheritance tells us this. But inheritance tells us this anyway.

6. Working on an SQLite DB program to see how SQLite APIs are transformed to libuv CBs.

-----------

10 Feb 2016

1. After a lot of wrestling, I've managed to convince node to install a development version of sqlite3 for my use. The issue was that unlike other modules I've installed (e.g. async), sqlite3 uses C++ bindings. The fix was to change the /usr/bin versions of nodejs to point to my development copy, and then to run 'npm install sqlite3' as usual. Strangely enough, I thought I'd done that already. I also installed node-gyp, not sure if that had an effect. 

There was advice online about using 'npm install --nodedir /path/to/devel/copy sqlite3' but this did not work. Perhaps I was using it wrong. The npm documentation feels wanting.

Now that I've got a working version of sqlite3 installed, it appears that my development version of node segfaults when running 'node sqlite_simple.js'. HOWEVER if I 'git branch master', the code runs fine. Interesting. My guess is that it's somehow related to using the C++ binding, as I haven't tried that before. 

Next step: attempt a C++ binding tutorial and see if that also segfaults on my version.

-----------

11 Feb 2016

1. I found the issue. The node-sqlite3 documentation wasn't quite up to snuff. Here's the secret sauce:

npm install --build-from-source --verbose --sqlite=/usr/local --nodedir=/home/jamie3/Desktop/node_project/node

The key was to add --build-from-source, which makes use of the src directory. Without it, any
local changes in src are just ignored.
I actually think the --nodedir is unnecessary since I've run 'make install' and installed the development
version on my system.
I made use of freenode's #node-dev IRC channel through ChatZilla to double-check that I was going through
the right motions (except for the --build-from-source). bradleymeck and evanlucas were helpful.

2. The trees arising from sqlite_simple.js are interesting. I'm going to make a collection of trees to walk through with Dongyoon today. I'd like to come prepared with a "demo" like the one I did today (see the weekly_meetings folder). It was really helpful for discussion.

3. Meeting with Dongyoon.
  We are 6 weeks out from the OOPSLA deadline. We'll see how development goes, but here are some tentative milestones:

-----------

12 Feb 2016

1. I've finally figured out how to add debug breakpoints to node.cc. In $NODE, run './configure --debug' and then 'make; make install'. This produces binaries called 'node' in $NODE/out/Release and $NODE/out/Debug. Use gdb on the Debug version. Then in gdb run 'break node::StartNodeInstance' or 'break node.cc:3972'. There's a symlink in ~/bin called node-debug for ease of use.

2. Explored promises for the ghost engine. I haven't finished learning about these but I'm making progress. An initial example is in /home/jamie3/Desktop/node_project/node/jamie/simple_node_programs/promises/promise_rsvp.js. See squid.ore email for reading material. My conclusion is that promises can be used asynchronously in a ``baton'' fashion, and that in the Ghost example they are used in that fashion. If *not* used in this fashion, they're basically being used incorrectly. Their purpose is to order asynchronous events; if each promise includes only synchronous code then there's no reason not to write it synchronously. Example:

if(x = 1).then(y = 2).then... 
  should be
x = 1
y = 2
...

-----------

15 Feb 2016

1. Resume client inheritance
2. Research timeline software

-----------

16 Feb 2016
17 Feb 2016

I spent these days learning about TheTimelineProj and developing a library to automatically generate XML for examination with it.
A default view was not useful because the range in time between events was too large.
Steps taken to address this:
  - normalization
  - era detection

-----------

18 Feb 2016

1. Complete working version of timeline interface. Still needs a true CLI. Use it to produce visualizations for identical executions against the three httpserver instances. timeline_demo is on my laptop. I've checked the timeline code into the node git repo on woody.

----------

19 Feb 2016

I spent today thinking about deterministic replay.

I wondered whether scheduling callbacks in pure JS seemed feasible.

I think to do so would require instrumenting the source code of any module that "rolls its own" libuv interactions. I don't like this idea, and it also introduces overhead (not as much as Jalangi, I think, but still non-trivial).

Scheduling them instead in libuv seems wiser. I think by disabling the V8 garbage collector we can safely retain references to callbacks forever, and only invoke them when we want to. In libuv we also have full control over the threadpool without having to worry about multiple components (JS and libuv) and communication between them.


----------

21 Feb 2016
22 Feb 2016

1. I attempted a first draft at a callback deferral mechanism. This version uses a scheduler thread. invoke_callback routes callbacks into a scheduler list for eventual execution. The scheduler thread picks callbacks off of the list and executes them.

Unfortunately this leads us to two related concerns. First, libuv thinks of itself as the bottom of the stack. Once a callback is invoked, libuv believes that it is done (and might proceed to free memory, dismantle the handle with which it's associated, etc.). If instead I've squirreled it away to execute later, that's a segfault waiting to happen. Second, once libuv thinks it is done, it may eventually (?) communicate to Node.js that the callback is no longer needed, and the V8 GC will scoop it up.

I do not think that pursuing this approach is worthwhile.

2. I pondered an alternative mechanism, still libuv-based. 
Suppose that in linux-core:uv__io_poll:
  1. Each pending fd was associated with exactly one "relevant" user-defined callback (e.g. ignore UV_ALLOC_CB and focus on UV_READ_CB) 
  2. We could determine the user-defined callback with which it was associated in a uniquely-identifiable way
Then we could defer invoking the UV__IO_CB whose turn has not come up yet, and have determinism without needing to worry about the V8 GC or the Node-C++ implementation.

Now, examining the timelines I've generated so far, I see that a single UV__IO_CB can actually wrap multiple user callbacks.
This occurs due to indirection in libuv. I think the indirection is to reduce the number of fds on which to epoll_wait. For example, instead of one fd for each async event, we have a single "async fd" and arm it when IO is observed on any of the async events. The specific fd(s) triggered is determined by the async handler. However, we can fudge the "ready" events to pinpoint the precise CBs we want to invoke out of the set of ready fds.

This approach seems much more promising. I'll have to think about the details further after lunch.
See jamie_libuv_src_notes for details.

----------

23 Feb 2016

1. Lots of pondering in jamie_libuv_src_notes regarding the feasibility of my second approach.
2. I think I've reached a good point with my second approach. It'll require a decent amount of mucking about in libuv, but it seems sound.
   I think I should try to implement a first draft using something easy -- timers.
   Based on sample_prog timers.js, the only CBs that are triggered in a timer-only workload are UV__IO_CB and UV_TIMER_CB. This is as expected.
   A quick count suggests that timer callbacks are coalesced by Node. A poke around node/lib/timers.js and IRC suggests that:
    1. Timers are indeed coalesced
    2. These are the only callbacks coalesced above libuv

    Commit 7d2b9009 disables timer coalescing, at least roughly.

----------

24-29 Feb 2016

1. I've implemented the new tracking scheme (Logical Callback Nodes - LCBNs) alongside the original CBN code.
   I figured why delete the CBN code unless I was sure I wouldn't need it?
2. I've done refactoring and improvements (e.g. unified-callback-enums.[ch]).
3. I've moved the graphviz stuff into Python. All I need is a printout of the LCBNs and I can produce a tree. This is better; with a constant input file I can tweak the display. More modular.

----------

29 Feb 2016

1. All threadpool code is now routed through uv_queue_work. This required adding wrappers in fs.c, getaddrinfo.c, and getnameinfo.c.
2. Begin work on dependencies (nodes other than the registering parent that precede a node). Plot these in Python using dashed lines.

---------

2 Mar 2016

1. LCBN order info: differentiate between registration order and execution order.
2. Don't track the dummy CB from uv_try_write. If it gets executed we're doomed anyway.

---------

3 Mar 2016

1. LCBN tracking: WORK -> FS_WORK -> AFTER_WORK -> FS_CB, etc.
    Note that this is a bit misleading; unlike all(?) other LCBNs, WORK/FS_WORK and AFTER_WORK/FS_CB (etc.)
    cannot be executed independently. They are a unit. Scheduling logic will need to take this into
    consideration.
2. Record registration time for comparison with execution start.
   If the gap between registration time and execution start grows over time, this
   implies that the threadpool is overloaded and should be re-sized. Could be used
   to improve Node performance. libuv devs have been thinking about this since ~2014:
     https://github.com/libuv/leps/pull/4
3. callbackVis: optional removal of un-executed nodes
4. CLI work:
    - Refactor TimelineCLI to use the CallbackNode class
    - One-shot CBN file -> timeline and tree

Next up:
  LCBN extraction: identify the LCBNs associated with run_timers/run_idle/etc. as well as with the fds found in uv__io_poll
    > Fill in APIs like uv__ready_timers. Do depth rather than breadth, agile. Start with timers, then check/idle/prepare, then...

---------

4 Mar 2016

1. Mock up a scheduler API and implementation. Not yet complete. I think 1 more public APIC and 1-2 private APIs are needed; see TODOs in scheduler.[hc].
2. Rewrite uv__run_timers to use the scheduler APIs.

---------

7 Mar 2016

1. scheduler_init: REPLAY path (read from file; load desired_schedule)

---------

8 Mar 2016

1. scheduler: implement record path

---------

9 Mar 2016

1. scheduler: work through some bugs
2. schedule specification (record/replay and file) via env variables:
  UV_SCHEDULE_MODE
  UV_SCHEDULE_FILE

During testing I discovered that identifying nodes by <tree, level, entry relative to parent> won't work.

      O
     / \
    O   O
    |   |
    O   O  <-- these are both <0,2,0>, how to distinguish? 

The issue is that from <0,2,0> we don't know who the parent is.
Instead I think I must use <parent, relative child number> as the unique ID. This is more obviously correct.

3. Develop a tree class with which I can more cleanly structure the LCBN trees.
   See mytree.[ch].

---------

10 Mar 2016

1. Add more tree APIs and refactor LCBNs to use trees
2. Remove global_lcbn lists and replace them with counters
3. Refactor lcbn generating to use lists and a sort routine (on exec_id or reg_id)
4. Updated LCBN comparison (change API to lcbn_semantic_equals and comments)
5. Schedule input: rebuild tree so that lcbn_semantic_equals will work. This addresses the notes from 9 Mar.
6. Tweak scheduler's recorded_schedule to be the registration schedule, not the execution schedule. Now the schedule emitted by the scheduler matches the schedule parsed by the scheduler.

commit 582c888e3cf
I have tested the scheduler using this version
  - RECORD: execution of timer_tree
  - REPLAY: the schedule produced (arranged so that any WRITE_CBs occur after all TIMER_CBs finish, since we don't schedule WRITE_CBs yet)
  - REPLAY: a modified schedule (switching the order of two timer children)
It works!
Strangely, it doesn't work consistently when printing to stdout, but it works fine in gdb or when redirected to a file. Odd.

7. Include check, idle, prepare in scheduling (UNTESTED)
  - look in node.cc for commented-out code; add a few more and test it out

---------

13 Mar 2016

1. Test check, idle, prepare scheduling
  - used node.cc and verified that I can run two different orders of events
2. Refactored the ready_lcbn routines to indicate the execution_context in which
  we need to determine the ready LCBNs. Preparation for uv__run_closing_handles.

---------

Mon 14 Mar 2016

1. uv__run_closing_handles
    This is now converted to use the scheduler. Not all handles survive the trip.
    For example, if you run httpserver_background.js + http_client.js (RECORD) and then http_client.js (REPLAY),
    it will assert because there is no uv__ready_tcp_lcbns to tell us whether or not we should run uv__finish_close
    on the client's TCP handle.

---------

Tue 15 Mar 2016

Nothing, worked on cloud computing project automation all day

---------

Wed 16 Mar 2016

1. Add scheduler ready_lcbns stubs for all handle types. These stubs do the basic portion of CLOSING_HANDLES.
2. Support uv__run_closing_handles completely.
3. uv__run_pending: support UDP handles, stream (tcp, pipe, etc.)

uv__run_pending
 - Thought a bit about this one. It's of the same type as uv__io_poll: it deal in uv__io_t's rather than handles or requests directly.
   Will need an extra layer of abstraction to identify the target CB (which tells us the type of the wrapping handle or request) and extract LCBNs appropriately.
   More complex, so do it after uv__run_closing_handles. The same code should work for uv__io_poll as well.

---------

Thu 17 Mar 2016

1. Complete uv__run_pending schedule implementation
  - finished uv__stream_io
  - finished handle extraction

  > NB Here (as in uv__run_closing_handles) we see multiple LCBNs that may be invoked. 
    Once I call e.g. uv__{stream|udp}_io,
    all of those LCBNs will be invoked, whether or not they were actually next.

    HYPOTHESIS:
    I believe that in a correctly-described replay these LCBNs will be in 'next' order,
      i.e. if ABCDE will be invoked then the schedule should read ABCDE.
    If replaying an unmodified schedule, this should work every time.
    However, this requires that we identify the "clusters"/"streams" of events that are all-or-nothing,
    a la uv__stream_io, and do not interrupt those clusters with other events.
    I added an assert to detect anything unexpected in this case.
 
    FOR CONSIDERATION:
    Per discussion with Dongyoon:
      - UV_READ_CB will be called when data is ready, whether or not
        it's semantically complete from the user's perspective.
        Consequently, we might invoke a READ_CB at the correct point,
        but nread might not equal what was read in record mode.
        If we read too much, no backsies. If we read too little, we could
        spin, I guess. Anyway we may want to consider this.
        We'll see what this looks like from Node's perspective.

---------

Fri 18 Mar 2016

1. uv__io_poll: hook into scheduler
  - still missing the ready_lcbns funcs

---------

Mon 21 Mar 2016

1. Ready_lcbns for uv__io_poll stream functions: uv__stream_io (output), uv__server_io (input)
  > tested (notes on 3/21)
2. One thread, scheduled threadpool (work only)

---------

Tue 22 Mar 2016

1. Fix async bugs
2. Full scheduler support for threadpool
3. mutex for LCBN invocations in invoke_callback

---------

Wed 23 Mar 2016

1. Document async info
2. Research GUI editors for graphviz files
3. Update cbGraphVis to show 'timeline-friendly' version
  ./cbGraphVis --execOrder ...
  Nodes are graphed with vertical distance indicating execution order.


  - Successfully demonstrate re-ordering(s) for fs_nondet.js
    (need to develop schedule re-ordering tools)

---------

Thur 24 Mar 2016

1. Identify an easier-to-reorder FS case: fs_nondet_stat.js
  - stat, unlink both incur a single WORK/AFTER_WORK cycle, rather than 3 or 4 like write/read
  - success: ~/Desktop/node_project/node/jamie/weekly_meetings/mar24_2016
2. uv__io_poll, uv_run: instead of hanging at epoll() at end of schedule, just exit(0). This won't work for applications that have listeners that go off when the 'exit' event is emitted, but it's certainly convenient for now.

---------

Mon 28 Mar 2016

1. Tracked down the source of intermittent UV_WRITE_CBs. I changed the Node code so that every call to console.log results in a UV_WRITE_CB.
2. I refactored mylog to have log classes and verbosity levels.
3. I fixed a bad assert in scheduler.c and a missing map_looks_valid in map.c. These were causing issues.

Pending problem:
  When I run fs_several_stat.js > /tmp/out, no WRITE_CBs are registered.
                                | tee /tmp/out, WRITE_CBs are registered in the correct order.
                                under gdb, WRITE_CBs are registered in the correct order.
                                with no redirection, WRITE_CBs are registered in an incorrect order

    Commands: 
      UV_SCHEDULE_MODE=RECORD UV_SCHEDULE_FILE=/tmp/fs_several_stat.sched node fs_several_stat.js
      scpdown /tmp/fs_several_stat.sched .; ./cbGraphVis --onlyExecuted --execOrder fs_several_stat.sched fs_several_stat.gv

  In addition, when WRITE_CBs are registered in the correct order, REPLAY only sometimes seems to go through properly.
  If I disable mylog altogether, RECORD sometimes shows a final ASYNC_CB. This will sometimes flummox REPLAY.

---------

Tue 29 Mar 2016

1. I (think?) I've eliminated the "missing WRITE_CB" problem. Essentially I went through and fixed all of the compiler warnings, and fixed a few bad casts in the process. I guess one of them was threadily damaging the input CB.
2. I've studied the problem in more detail. See notes in node_src_notes.

OPEN PROBLEMS:

1. UV_WRITE_CB
  With redirection to a file, no UV_WRITE_CBs at all are registered.
  Without redirection to a file, UV_WRITE_CBs are registered, but parentage varies between UV_FS_CB and UV_WRITE_CB.
  Example:
    (16:44:04) jamie@suwarna7-Lenovo-K450e ~/Desktop/node_project/node/jamie/simple_node_programs $ UV_SCHEDULE_MODE=RECORD UV_SCHEDULE_FILE=/tmp/fs_several_stat.sched node fs_several_stat.js  > /tmp/out
    (16:44:24) jamie@suwarna7-Lenovo-K450e ~/Desktop/node_project/node/jamie/simple_node_programs $ grep WRITE /tmp/fs_several_stat.sched
    (16:44:25) jamie@suwarna7-Lenovo-K450e ~/Desktop/node_project/node/jamie/simple_node_programs $ grep APP /tmp/out
    APP: 1: Wrote file
    APP: 2: Read file
    APP: 3: Wrote file
    APP: 4: Read file
    APP: 5: Wrote file
    APP: 6: Read file

  cf. node_src_notes, but since the underlying stream type changes depending on whether or not I use redirection, this seems reasonable.
  
  Next steps:
    - Still unsure about root cause. I suspect the cause is buffering at the Node-JS or Node-C++ layer.
      Either way this is unpleasant, since it implies that similar library buffering could be a problem in other settings.
    - Identify the path taken when output goes to a file (gdb blah; run fs_several_stat.js > /tmp/out)
    - Remove the buffering at Node-C++ or Node-JS level.

  Why is this a problem? It means that on replay the behavior might diverge due to timing outside of my control.
  I either need to get the timing under my control, or I need to add a "give up" mechanism instead of bailing out in invoke_callback
    if sched_is_next_lcbn in not true.

2. UV_ASYNC_CB
  In fs_several.js, UV_ASYNC_CB shows up as the final item in RECORD, but REPLAY hangs without getting to it.
  I'm guessing I'm missing a uv_async_send somewhere?

---------

Wed 30 Mar 2016

1. Reasonable overhaul and debugging for UV_WRITE_CB. 
   When ssh'ing into woody from my laptop, the UV_WRITE_CB bug seems to be gone.
   We'll see if it's just an ssh session (timing?) thing tomorrow when I'm in the office. 
   But for now let's proceed.

2. A variety of tests now pass. I fleshed out test_loop a bit. 
   fs_several fails with trailing UV_ASYNC_CB, need to figure this out.

---------

Thur 31 Mar 2016

1. I solved the trailing UV_ASYNC_CB bug. It was a benign race condition 
  between the threadpool threads and the looper thread.
  See commit message in 893f80c3a for details.

2. I developed a run_tests wrapper to run tests in RECORD and REPLAY mode,
delivering SIGINT for me. Requires the test to be self-driven and to
complete within ~2 seconds. See 47cb3cdb0d7a7.

3. Enable -Werror. Resolve all compiler warnings.

---------

Fri 01 Apr 2016

1. Running run_tests repeatedly, I've fixed a few race conditions/deadlocks in REPLAY mode. One more TODO regarding parallel LCB executions that I haven't yet resolved, though I know how to do so.
2. I've tried running run_tests nohup'd, but something seems to go wrong. Using screen and trying a "big test":

screen
./run_tests  --redirect --niter 100 dns_lookup_many.js dns_lookup.js fs_read_nest1.js fs_read_nest2.js  fs_write_nest1.js fs_write_nest2.js fs_write_nest3.js fs_nest.js fs_nondet.js fs_nondet_stat.js | tee /tmp/run_tests_output
Use 'Ctrl-A d' to disconnect

Use 'screen -r' to re-attach.
http://www.tecmint.com/screen-command-examples-to-manage-linux-terminals/

   - httpserver_complex_schedule fails. I must explore this further.
   - need to try MUD too

RESULTS:
  - passed with 100 iter on each (took about 2 hours)

NEXT STEPS: 
  - I concocted a more competitive fs_nest_many.js, which consistently causes the dread jump to manifest regularly as follows:
    UV_SCHEDULE_MODE=RECORD UV_SCHEDULE_FILE=/tmp/schedule_file.sched node fs_nest_many.js

    ** This only happens regularly while in a screen session. Timing... **

    You can also reproduce the issue with: ./run_tests --niter 20 dns_lookup_many.js
      This actually produces issues like

      MAIN       3   Mon Apr 04 20:16:29.213156708    20726   140676564018944      invoke_callback: invoking lcbn 0x298ac00 (type UV_GETADDRINFO_WORK_CB) context 0x298aa48 parent 0x29021b0 (parent type INITIAL_STACK) type 
                           This actually produces issues like0x298b300 lcbn_orig (nil)

  which is real-live corruption instead of just weird missing newlines. Promising!
  Anyway, this regularly errors out appropriately, and both in RECORD (doesn't error out, just completes somewhat as expected) and REPLAY (does error out) mode.

-----

I installed gcc-4.9 which has support for -fstack-protect-strong. I mv'd /usr/bin/cc to /usr/bin/cc-orig and pointed cc to gcc-4.9.
I added -fstack-protect-strong to uv's uv.gyp.

---------

Mon 11 Apr 2016

- ASPLOS was fun!
- I've been fighting with the "weird output/misbehavior" bug for about 2 weeks. I think it's gone. Here's what I learned:

1. Always compile with -Wall -Werror and fix the compilation errors. The compiler is your first line of defense.
2. Test all system calls for errors, including printf/fprintf
3. Always write unit tests for your code modules. Even if it seems silly or needless. It pays off big-time in debugging later on.
4. Be liberal in the use of log() calls. More output is always helpful.
5. If you use any_func-style function pointers (void (*any_func)()), cast them to the appropriate type before you invoke them.
   This allows the compiler to protect you.
6. It's hard to get thread synchronization to work properly! As Knuth advises, premature optimization is unwise.
   Stick to pthread_yield() unless you have hard evidence that the performance suffers.

Sources of the problem:

1. printf/fprintf was failing with EAGAIN and I was ignoring it. Given the gaps in execution time I observed, I suspect it was failing repeatedly.
   The "execution jump" was really a long sequence of failed printf's.
2. Cast function pointers appropriately.
3. The hang was due to a deadlock elsewhere in the code. I didn't realize this because the output looked so weird that I couldn't tell where the execution was hung.

---------

Tue 12 Apr 2016

1. Tweak timing, output, etc. so that run_tests passes consistently
2. New test programs:
  ad_hoc_sync_timer.js          demonstrates ad hoc synchronization using a timer
  http_closed_system_easy.js    server and client within a single JS program. Easy schedule (no async).
  *http_closed_system_medium.j   sserver and client within a single JS program. Medium schedule (server does FS op before reply)
  *http_closed_system_hard.js    server and client within a single JS program. Hard schedule (like medium, but with background noise: DNS, timers, FS)

  *Does not pass run_tests yet

---------

Wed 13 Apr 2016

1. Add more logging throughout.
2. Add safe variable declarations.

- Working on understanding the error in http_closed_system_medium.js (224 CBs run, hangs looking for a SHUTDOWN_CB with 71 CBs left).

---------

Thur 14 Apr 2016

  - study how event emitters are implemented in Node

--------

Fri 15 Apr 2016

1. Pushed the git repo to bitbucket for DY to try his hand at things.
  Branch: cbn_remove
  To push the latest: 
    git push --mirror git@bitbucket.org:jamiedavis314/nodejsrr

2. Begin marker events for high fidelity recording.

--------

Sat 16 Apr 2016

1. Tweaks to marker system for correctness. Works my hand. 
   Overnight run: screen session is running http_close_*
   If this works, wrap it up into a commit.

--------

Sun 17 Apr 2016

The overnight runs passed. Yay!
Morning run: regressiontest (everything that has passed so far)

I tried nodejs-mud over VNC but it hung in the same fashion that it did before the introduction of the marker events. VNC is pretty slow. Debugging would be easier from the office, but before then...
  - GUI interactions are hard
  - It would be nice if I could mimic the interaction through an http_close type test case. Study the behavior and add a new test case.

--------

Mon 18 Apr 2016

1. Progress on debugging nodejs-mud:
  - Fix more bugs in marker code, added some asserts.
  - Identify causes of sometime-failures in multi-client nodejs-mud recordings:
    A. The read requests actually differ. On record, sometimes favicon is not requested. On replay, sometimes it is.
      I'm guessing this is a browser state issue, so I need to be more careful about resetting the state.

      In addition, the favico request is sometimes of a different length, so perhaps adding length checks
      on reads wouldn't be safe? Need to check. Compare the 'Accept:' field:
        UV_STREAM  9   Mon Apr 18 11:23:40.004977512    18718   140092208760704      mylog_buf: Buffer 0x2945560, len 304: GET /favicon.ico HTTP/1.1
        Host: localhost:50034
        User-Agent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:44.0) Gecko/20100101 Firefox/44.0
        Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
        Accept-Language: en-US,en;q=0.5
        Accept-Encoding: gzip, deflate
        Connection: keep-alive

        vs.

        UV_STREAM  9   Mon Apr 18 11:23:10.006021579    18708   140012807735168      mylog_buf: Buffer 0x3525630, len 274: GET /favicon.ico HTTP/1.1
        Host: localhost:50033
        User-Agent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:44.0) Gecko/20100101 Firefox/44.0
        Accept: image/png,image/*;q=0.8,*/*;q=0.5
        Accept-Language: en-US,en;q=0.5
        Accept-Encoding: gzip, deflate
        Connection: keep-alive

    B. The order of requests sometimes varies. 
       Each client opens two streams. I hypothesize one for AJAX queries (GET /recv) and one for regular queries (GET /, /style.css, move).
       However, the type of request that goes through each stream seems to vary.
       This might simply be due to timeouts because the VNC connection is so slow.
       Need to verify this behavior when in the office.

--------

Tue 19 Apr 2016

1. I added extra_info to track the fd on which a READ is performed.
2. I believe I have the answer to the nodejs-mud record/replay mystery. 

HTTP/1.0 is a stateless protocol. 
This means that there are not persistent client connections; each HTTP request is done on its own connection.
HTTP/1.1 has a keep-alive option, which is implementation dependent and lets you avoid making unnecessary connections.

Issues with nodejs-mud that prevent successful record/replay:
  It uses HTTP and encodes client information in the GET request. 
  The GET requests may come in on a variety of sockets, with moves predominantly on the same socket,
  because from a protocol point of view they are a series of "idempotent" unique client interactions.

  It seems difficult or impossible to record and replay these by hand.

  If the application itself is recorded (or automated), we might be in business, but doing it by hand
  leaves you at the mercy of games the web browser or http server might play.

  - This is why it (mostly) works with a single user -- less nondeterminism.
  - However, even with a single user it sometimes fails. This is due to:
    > Sometime-retrieval of favicon
    > Non-deterministic first retrieval of board and registration

3. I developed a nicer record/replay client system than the one from the Jalangi benchmarks.
  (21:50:00) jamie@suwarna7-Lenovo-K450e ~/Desktop/node_project/node/jamie $ ls ~/PycharmProjects/ClientDriver/
  mudClient.py

  This version runs multiple clients with random queries.
  You can provide a seed to get reproduceable results.

  Once the initial "loading" queries have completed, the order of the requests are indistinguishable.
  It looks like a single fd suffices to handle all of them, perhaps 
    because the HTTP requests are being launched synchronously and therefore there is *never* more than
    one pending (non-keep-alive?) request at epoll time? 
    I'm guessing the extra sockets were spun up by the Node.js HTTP libraries in response to keep-alive connections.
  Anyway: I can record and replay mudClient.py. Huzzah!

4. I added --adjList to cbGraphVis for Arun's combining pleasure.

5. I added ALLOC -> READ edges for the non-initial pairs. Before, I was only tracking the dependency for the initial pair.

--------

Wed 20 Apr 2016

1. I abstracted my client system a bit. I defined the interface that client drivers need to support and wrote a wrapper clientDriver script.
2. I began my study of process.nextTick.

--------

Thur 21 Apr 2016

1. I completed my study of process.nextTick. 

  It turns out I was reading the documentation a bit wrong. See node_src_notes for details, but essentially:
    process.nextTick calls are synchronously handled after the end of the current stack.
    You can think of them as wrapping the current stack in a larger function call that goes:
      - call function
      - empty any queued nextTick functions
    This is done recursively, so if you nest nextTick calls you are basically putting your code into a while(1) loop.
    The function name indicates that the next tick of the loop will be to run the provided function rather than to proceed as normal. 

    This means that we don't need to worry about process.nextTick as a source of nondeterminism, since the nextTick functions
      are called synchronously as "part" of the original function call.

2. I completed my study of setImmediate. See node_src_notes for details.

  Essentially, when you call setImmediate(CB), CB is placed in a list of 'immediate CBs'.
  All of these are executed in a single UV_CHECK_CB when next that portion of the event loop is reached.
  If a setImmediate CB calls setImmediate, it gets queued up for the next time through the loop (forward progress).

  I updated set_immediate.js to demonstrate this behavior.

--------

Fri 22 Apr 2016

1. I verified that setImmediate(CB) works as I thought, by examining the graph of set_immediate.js.
2. Studying how Promises work.

Reading material: 
https://strongloop.com/strongblog/promises-in-node-js-with-q-an-alternative-to-callbacks/
  - quick tutorial for the Q promise library
  "If you return a promise, it will signal the next then when the asynchronous operation completes. You can also return any other value and the next onFulfilled will be passed the value as an argument"
  This means: if you return an (asynchronous) promise, it will call the next 'then' when the async operation completes.
              otherwise, the next 'then' will get the return value as input synchronously.
  Therefore, from an implementation perspective, we can think of promises as a way to organize control flow,
  but they work "just about as you'd expect". They are just glue to make sure that flow ordering is correct.

https://blog.jcoglan.com/2013/03/30/callbacks-are-imperative-promises-are-functional-nodes-biggest-missed-opportunity/
  - long, but a great explanation of callbacks vs. promises

In short, I don't think we need to worry about the use of promises -- we should get a nice CB happens-before ordering from such chains.
Promises are orthogonal to setImmediate.

  TODO: This means that the first guy to call setImmediate when there are no setImmediate CBs pending gets labeled
        as the parent of the UV_CHECK_CB. Subsequent callers have their CBs embedded within it but the parentage information
        gets lost.

        The loss of parentage information is bad. Replace the single wrapper with per-caller uv_check_t handles, similar to what I did
        for timers. Use timer_wrap.cc as a template.

--------

Sun 24 Apr 2016

1. I implemented promise_q.js using the BlueBird library as promise_bb.js. The two CB graphs are identical with regard to executed CBs, though
   there is some mismatch in non-executed CBs so the schedules are not interchangeable. That's to be expected I guess.

   My conclusion is that promises.then, like async.waterfall, give us clean-cut async CB graphs. No worries there.

2. I studied the async module a bit.

   promises:then :: async:waterfall, and you'll get the same CB chain. 
   promises:all  :: async:each/parallel

   Consider the CB graph produced by promise.all/async.each

   Callers of async.each are claiming that the CBs can be invoked in any order with no negative impact.
   The result is a graph as follows

     caller of async.each
       /   /   |   \   \
     f1  f2   f3   f4  f5
      .  .  .  .  .  . 
      .  .  .  .  .  . 
          \ 
         last one to finish continues the thread of execution by executing the provided DoneCallback (D-CB)

  Now, two comments here:
    i.  The 'last one to finish' model means that under a modified schedule the tree may change shape but not meaning.
        e.g. If f5 finished last and I want to make it so that f4 finishes last, we'll have f4 -> D-CB instead of f5 -> D-CB.

        f4 -> D-CB and f5 -> D-CB are fundamentally the same schedule, in the sense that D-CB will be the child of 'the last one', 
          whichever that might be, rather than of a specific one across every execution.

    ii. The claim that the CBs can be invoked in any order with no negative impact bears testing. We have no reason to take the
        developer at his word on this subject.

    This leads us to the following problem:
      - we want to re-order the CBs, and this is entirely legal
      - re-ordering the CBs may result in a "different" schedule as far as the parentage of D-CB is concerned
      - but we want to think of the difference between the schedules as only having flipped the work CBs, not as having 
        moved into a new schedule and having to switch into RECORD mode.

    Can we re-order the CBs and correctly infer the "rule" for how D-CB is invoked?
      Same CB type, different parent, common ancestor; "brothers" with a shared child.
        *and brothers with the child belonging to the most-recently-finished (i.e. the oldest child)
      The async.each() CBs need not be identical in structure, so identifying the "brothers" could be tricky.

      We can climb the tree and find the ancestor all of whose children have most recently finished, and if we were the final child
        we could conclude that we are in an async.each-type situation.

    Stated more generally, the problem is in identifying higher-level ordering constraints at the lower level.
    Obviously there is no clean solution without having knowledge of the higher level, but we could perhaps develop some heuristics, as explored above.
 
3. Initial implementation of setImmediate: I just tweaked it to use 0-length timers, since timers are no longer batched.
   This approach has some drawbacks, namely:
    1. Change from regular libuv order: setImmediate used to go off in CHECK, prior to uv__run_closing, and now goes off in uv__run_timers.
    2. Lack of guarantee about 'setImmediate's go off in FIFO order'.

--------

Mon 25 Apr 2016

1. Rework setImmediate as a proper per-call uv_check_t. Overhead up, but graph visibility up.
   As far as I know, this is the last remaining place where CBs are batched by Node in an unexpected fashion.
   EventEmitter listeners are similarly batched, but that is well understood.

2. I explored the lighter application. Between 'start' and 'poll waiting for user input' there are plenty of CBs.
   RECORD worked fine. REPLAY was a bit iffier. It looks like the startup phase involves some non-determinism,
   and also that this non-determinism is somehow related to timing.

   I think this is demonstrated by the following: RECORD x2 (the first one primes the caches) followed by N REPLAYs:

  Terminal 1:
    export N_RECORD=2
    export N_REPLAY=100
    echo "RECORD x $N_RECORD"; 
    for i in `seq 1 $N_RECORD`; do echo "  record $i"; UV_SCHEDULE_MODE=RECORD UV_SCHEDULE_FILE=/tmp/f.sched  /home/jamie3/bin/node server.js lighter > /tmp/rec$i 2>&1; done; 
    echo "REPLAY x $N_REPLAY"; 
    for i in `seq 1 $N_REPLAY`; do echo "  replay $i"; UV_SCHEDULE_MODE=REPLAY UV_SCHEDULE_FILE=/tmp/f.sched  /home/jamie3/bin/node server.js lighter > /tmp/rep$i 2>&1; done; 
 
  Terminal 2:
    export N_RECORD=2
    for i in `seq 1 $N_RECORD`; do kill -s USR2 `tail -1 /tmp/rec$i | awk '{print $7}'`; sleep 1; done

  This works every time.

  Without running them in quick succession (does some daemon like mongoDB void its cache every X seconds?), the read() calls mismatch.
  Try code like this to check that quickly (perhaps adding a '| sum' to the end if the sequence is quite long):
    grep '= read' /tmp/rec1 | tr -s ' ' | cut -d ' ' -f12-14
  or
    for f in /tmp/rec /tmp/rep; do grep '= read' $f | tr -s ' ' | cut -d ' ' -f12-14 | sum; done

  Conclusion: I have mixed feelings about this. 
  On the one hand, it's cool that I record *and replay* lighter, and that's a good sign for my work.
  On the other hand, it's unfortunate that *without any user input* there can still be replay failures due to 3rd-party modules, caching, and so on.
  But as a targeted test tool, the developer can control any inputs using scaffolding. So I guess this might still be OK.

--------

Tue 26 Apr 2016

1. Add --colorFile to cbGraphVis. This way when Arun says two items raced, I can visualize and think about it as needed.
2. I observed that 'npm install X' now takes far longer than when using the distribution version 'PATH=/usr/bin:$PATH npm install X'.
   I spent most of the day profiling my code and eliminating hot spots.

   Results at end of day:

   Using http_closed_system_huge.js with nclients = 1000:
    Mine:
    real	0m15.284s
    user	0m15.035s
    sys	0m0.265s
    (all clients submit requests synchronously, then server tries to handle one of them and it times out)

    Distr:
    Runs for 30 seconds, successfully handling all of the clients

    Why don't the distr clients time out?

                    Time solely to submit requests for varying num clients (use 'time', and put invalid JS code at the bottom of the file, to be executed after submitting the requests)
    Node version	 1000			 2000
      mine		      11.9 s		14.2 s
      distr		      5.7 s		  6.9 s

    This suggests that my version of node handles client requests at a rate roughly twice as slow as the distr. version.
    With n_clients = 100, my version doesn't time out, though of course it's still slower.
    Notes on the profiling process are in profiling_notes.

    Now, of course *some* slowdown is expected. Recording every user CB adds overhead.
    However, 'npm install sqlite3' still takes forever.

3. For an immediate workaround, I added a process.hrtimeWall function. 
   This exposes us-granularity timestamps to JS, with the same interface as process.hrtime. 
   It should allow us to test the combined system using native Node.js APIs and JS-only modules until I figure out why I can't npm install using my own version of node.

--------

Wed 27 Apr 2016

1. profiling: I compiled the notes and reflected on my experience on my blog.

  Short version:
  valgrind --tool=callgrind --callgrind-out-file=callgrind_out COMMAND

2. Since the schedule is emitted atexit:
    - I updated the test programs to exit by default, with '--forever' suggesting they block on stdin
    - I updated my regressiontest tool to work much more quickly because can just wait until the RECORD exits instead of sleeping for 10-30 seconds and then signaling to dump

   I ran regressiontest for 100 iters to check for regressions introduced by my performance improvements. Got 'em all.

3. Profiling 'npm install' in search of performance issues, as follows:

  rm -rf node_modules/sqlite3
  UV_SCHEDULE_MODE=RECORD UV_SCHEDULE_FILE=/tmp/f.sched valgrind --tool=callgrind --callgrind-out-file=callgrind_install_mine npm install sqlite3

  sleep 30
  ctrl-C it

4. Tweaked cbGraphVis for Arun. Can now obtain only an adjacency list, without the expense of producing the .gv file if not needed.

5. Compared the install behavior using /tmp/node_basis vs. ~/Desktop/node_project/node.

   Latest version of my node:                               /usr/bin/node-devel
   Version of node from 'master' when I checked it out:     /usr/bin/node-devel-orig
   Distribution version of node:                            /usr/bin/node-distr

   Use something like
     sudo unlink /usr/bin/node; sudo ln -s /usr/bin/node-devel /usr/bin/node
     sudo unlink /usr/local/bin/node; sudo ln -s /usr/bin/node /usr/local/bin/node
   to change which version of node is used.
     (npm uses /usr/local/bin/node)

   Then npm install like:
     export UV_SCHEDULE_MODE=RECORD; export UV_SCHEDULE_FILE=/tmp/f.sched

     export MOD=sqlite3; rm -rf node_modules; npm --loglevel verbose install $MOD
     export MOD=async; rm -rf node_modules; npm --loglevel verbose install $MOD
     export MOD=commander; rm -rf node_modules; npm --loglevel verbose install $MOD

   Recipe:

    REDIRECTED:
     set -x
     cd /tmp/bb
     for NODE_VERS in node-devel-orig node-devel; do
       for MOD in sqlite3 async commander; do
         echo "NODE_VERS $NODE_VERS MOD $MOD"
         sudo unlink /usr/bin/node; sudo ln -s /usr/bin/$NODE_VERS /usr/bin/node
         sudo unlink /usr/local/bin/node 
         sudo ln -s /usr/bin/node /usr/local/bin/node 
         export UV_SCHEDULE_MODE=RECORD; export UV_SCHEDULE_FILE=/tmp/f.sched
         rm -rf node_modules;
         npm install $MOD --loglevel verbose > /tmp/$NODE_VERS-$MOD 2>&1
       done;
     done;
     set +x

    NOT REDIRECTED:
     sudo script
     set -x
     cd /tmp/bb
     for NODE_VERS in node-devel-orig node-devel; do
       for MOD in sqlite3 async commander; do
         echo "NODE_VERS $NODE_VERS MOD $MOD"
         sudo unlink /usr/bin/node; sudo ln -s /usr/bin/$NODE_VERS /usr/bin/node;
         sudo unlink /usr/local/bin/node 
         export UV_SCHEDULE_MODE=RECORD; export UV_SCHEDULE_FILE=/tmp/f.sched
         rm -rf node_modules;
         npm install $MOD --loglevel verbose
       done;
     done;
     set +x

     ctrl-d to exit
     examine file 'typescript'

   Result:
    - The devel versions seem to go through a different sequence than the distribution version.
      It looks like the distro version copies a local version from ~/.npm, while the devel versions pull it from online.

    When output is redirected
      - node-devel-orig and node-devel both work fine
    When output is NOT redirected (i.e. goes to stdout)
      - node-devel-orig works fine
      - node-devel errors out with what looks like a race condition?

        npm verb gentlyRm don't care about contents; nuking /tmp/bb/node_modules/async
        npm verb tar unpack /home/jamie3/.npm/async/1.5.2/package.tgz
        npm verb tar unpacking to /tmp/bb/node_modules/async
        npm verb gentlyRm don't care about contents; nuking /tmp/bb/node_modules/async
        npm verb stack TypeError: Cannot read property 'call' of null
        npm verb stack     at Timer.wrapper (timers.js:279:18)
        npm verb cwd /tmp/bb
        npm ERR! Linux 3.13.0-79-generic
        npm ERR! argv "/home/jamie3/Desktop/node_project/node/out/Release/node" "/usr/local/bin/npm" "--loglevel" "verbose" "install" "async"
        npm ERR! node v5.0.0-pre
        npm ERR! npm  v2.14.3

        npm ERR! Cannot read property 'call' of null
        ...
        npm verb stack Error: Callback called more than once.
        ...

      This smells like a null pointer dereference.
      I'm pretty confident that my version of node doesn't do anything illegal.
      Perhaps, then, the changes in execution order resulting from my tweaks are triggering a race.
      If so, that would be a swell example of a race that our project can find and help fix.
        However, it happens every time, so it's not like I can brag about record/replay playing a role here...

6. Installed sqlite3, then ran my sqlite_simple.js app. Success!

    With /usr/bin/node -> node-devel-orig
      rm -rf node_modules/; npm install sqlite3 --build-from-source --nodedir=~/Desktop/node_project/node 2>&1 | tee /tmp/out 

    I'm still having trouble getting my version of node to install things properly. 
    The bug above (my fault?) happens sometimes, and other times it crashes in uv__register_callback with no current LCBN (my fault!).

    Ah well. The best workaround is, as stated above, to set the system version of node to a working copy with the same version (i.e. based on /tmp/node_basis).

    The --build-from-source flag is undocumented in the npm manual and is recommended in the README.md for the sqlite3 module.
    Without it, it appears that version of sqlite3 'appropriate' to your version of node will be downloaded from the web.
    When used in conjunction with --nodedir, it says 'build from source, and link against the specified version of node'.
    For comparison, if you run 
      rm -rf node_modules/; npm install sqlite3 --build-from-source --nodedir=/tmp/node_basis 2>&1 | tee /tmp/out 
    and then node-devel, it segfaults.

    It is not clear how reliable the --build-from-source flag is, but given the experience with microtime (which doesn't even support development builds),
      it looks like only well-maintained C++-based modules will work. However, I wouldn't expect things like ghost or etherpad to use anything else.

    With --nodedir=~/Desktop/node_project/node, '/usr/bin/node-devel sqlite_simple.js' runs as expected.

--------

Thur 28 Apr 2016

Worked more closely with Arun today getting our systems together.

1. Hooked 'exit' events into the system.
2. Cleaned up git branches a bit. Still not perfect but...better...
3. Added multiple colors to cbGraphVis
4. Track and optionally remove non-user CBs (i.e. UV_ASYNC_CB for threadpool, but maybe more later).

screen: Running regressiontest to see

--------

Fri 29 Apr 2016

1. Continued systems integration
2. Minor performance improvements. regressiontest now passes.
3. Rework git repo.

--------

Sun 1 May 2016

1. Tweaked paths so that ~/Desktop/node_project/node now points to the noderacer2 version.
2. Improved the 'update' script with some error checking.

...
Worked like a madman until Sat 7 May when it became clear that OSDI was a loss.
...

--------

Wed 11 May 2016

1. Added registration time to the output of cbGraphVis's adjacency list mode
2. Added comments to and cleaned up raceDetector a bit.

Work in progress: processing the new adjacency list format in raceDetector.cpp
									May want to revisit the format I provide so that it's more easily extensible. Copy code from logical_callback_node

--------

Thur 12 May 2016

1. Some refactoring in reschedule. Defined a pivot. Still a work in progress.
2. Established that raceDetector.cpp has suffered a regression. Poked Arun about it.

--------

  VNC:
    ssh -p2000 -L 5900:localhost:5900 woody.cs.vt.edu #Set up port forwarding on 5900 to woody
    x11vnc -safer -localhost -nopw -once -display :0  #Set up a VNC server listening on localhost on woody.
    xtightvncviewer localhost:5900                    #Connect to it

Next steps:
  - regressiontest: Make sure we verify program output, too, not just libuv-level record/replay.
  - Incorporate DB example
  - Incorporate Promises example

--------

Mon 30 May 2016

1. Attempted to use DY's raceDetector. Emailed for help.
2. Developed test cases.

--------

Tue 31 May 2016

1. Figured out issue with DY's raceDetector. Updated cbGraphVis to use us instead of ns.
2. Worked on more test cases. 6 new ones are failing. Emailed DY about it.
3. Pushed latest progress so I can work on it from my laptop conveniently
  
--------------------------------

TODO Fill in the summer gap with notes on my laptop.

--------------------------------

Fri 26 Aug 2016

1. Finished up NEXT (for now) by printing a summary of what happened. Teach run_tests to check for test success by regex'ing the output of NEXT.
2. Wandered through online/analyses/Race.js. Did refactoring to improve how FS module race detection is handled. The new approach should make it easy to add support for other modules.

At the moment, if an application has racy async FS operations, raceDetector will hit an assert I added in July due to not yet pushing the "async-ness" into the appropriate child.
What about sync FS operations that occur in the same CB? e.g.
  var fs = require("fs");
  fs.mkdirSync("/tmp")
  fs.mkdirSync("/tmp")

--------------------------------

Mon 29 Aug 2016

1. Start working on pushing down async races

--------------------------------

Tue 30 Aug 2016

1. Continue working on pushing down async races

--------------------------------

Wed 31 Aug 2016

1. Refactor Jalangi analysis. 
  - Change "Race.js" to "Trace.js" (since we aren't doing race detection anymore, just read-write tracing).
  - Remove a bunch of code we aren't using.

--------------------------------

Thur 01 Sep 2016

1. Remove a ton of unnecessary code from Trace.js (now "TraceRW.js").
   Also, refactor out the "logger" and "trace fs" into sub_modules.
   And determine how to create non-enumerable (i.e. "hidden") object attributes.
   Our analysis is now slim and sexy.
2. Work on compiler errors in raceDetector.

--------------------------------

Fri 02 Sep 2016

1. Finish async push down in raceDetector

--------------------------------

Mon 05 Sep 2016

Happy Labor Day!

--------------------------------

Tue 06 Sep 2016

1. Work on rescheduler. Fix insertion bug in Schedule::_updateCBTreeWithTPDoneStage().

rescheduler now runs fine, but the racy schedule doesn't "work" for tests/modules/fs/fs-mkdir.js.
In particular, the cbGraphVis of the racy schedule looks the same as the original schedule, just with the "right-hand" tree occurring even later.
  - Does cbGraphVis flip the left and right sides? Why?
  - If not, why aren't the LHS and RHS in the right exec order?
      > add assert in Schedule::reschedule() about pivot and exec IDs if there isn't such an assert already

TODO
  2. Try rescheduler using the async push down.
  3. Examine Jalangi trace to see how getFieldPre, getField, and invokeFunPre work together
     Can we simplify or improve?

--------------------------------

Wed 07 Sep 2016

Some reflection on project status (see progress report from 7 Sept.).
I don't think we can make EuroSys under our current approach.
I proposed a simpler "schedule fuzzing" approach to DY, who thought it seemed do-able.

1. Pondered general schedule design
2. Worked on refactoring to let us choose schedule at run-time.

See scratch sheet for next steps.

--------------------------------

Thu-Fri 08-09 Sep 2016

Work on schedule refactoring

--------------------------------

Sat 10 Sep - Sat 17 Sep 2016

Sick with strep throat, no progress

--------------------------------

Wed 21 Sep 2016

Hooray, I can (at last!) continue making progress

1. Finish work on schedule.[ch].
2. Refactor code to work with the new scheduler APIs (though it doesn't call them yet).
   - Yay, it compiles!
   - TODOs where the scheduler APIs should be called

--------------------------------

Thur 22 Sep 2016

1. Call scheduler APIs in threadpool worker
2. Implement Fuzzing Timer yield (RECORD mode only)
3. Experiment with random sleeps to see if we explore schedule appropriately 

--------------------------------

Fri 23 Sep 2016

1. Call scheduler_yield everywhere it's appropriate.
2. Register the looper thread
3. Attempt to reproduce both orders in fs-rw.js.

    fs-rw.js works like this:
      sync write of 'BEFORE'
      async read (first step is work item 0)
      async write (first step is work item 1) of 'AFTER'

    If I artificially delay the 2nd threadpool item (number 1), R -> W (R sees 'BEFORE').
    Otherwise, with the random delays introduced by timer fuzzing, W -> R (R sees 'AFTER').
    I've tried {MIN|MAX}_DELAY with shorter delays (u seconds) and longer delays (seconds).

    If I use the original node, I see either 'AFTER' or nothing (never 'BEFORE'). Some interesting thread interaction I guess.
      for i in `seq 1 100`; do UV_THREADPOOL_SIZE=2 UV_SCHEDULER_MIN_DELAY=50000 UV_SCHEDULER_MAX_DELAY=100000 node /tmp/test/fs-rw/fs-rw.js 2>&1 | grep BEFORE; done | wc -l

    My initial impression is that uniformly-sampled timer delays will not meaningfully affect execution.
    If I make one thread slower or sample from a non-uniform distribution, we might get more variation.

4. Discussion with DY:
    - Yes, we need a schedule point between threadpool and looper (i.e. in invoke_callback) because looper can be doing synchronous versions of the async work in the threadpool
    - He likes the idea of exploring timer fuzzing slightly more -- an anti-Gaussian (i.e. extremes) approach might turn up more interesting results
      > try this. an easy implementation is to decide to slow down random items dramatically and let everything else go through
    - He also likes the "degrees of freedom" idea, and suggests applying it in the TP with a single thread, and in uv__work_done to simulate different completion orders
      > try this too

5. Implementation of "slowing random items down dramatically" is a success. I can see the various interleavings manifested in the output.

   ####
   # fs-rw.js
   ####
   In fs-rw.js, 75% are 'AFTER' and 'BEFORE'/'' occur for a combined 25%.

   rm -rf /tmp/t; mkdir /tmp/t; ./online/install.sh -i tests/module_races/fs/fs-rw.js  -d /tmp/t
   rm /tmp/o; for i in `seq 1 200`; do UV_THREADPOOL_SIZE=2 UV_SCHEDULER_MIN_DELAY=100000 UV_SCHEDULER_MAX_DELAY=100000 node-devel /tmp/t/fs-rw/fs-rw.js >> /tmp/o 2>&1; echo $i; done
   grep APP /tmp/o | grep -c 'data <>'
     23
   grep APP /tmp/o | grep -c 'data <AFTER>'
     164
   grep APP /tmp/o | grep -c 'data <BEFORE>'
     13
  
   ####
   # fs-mkdir.js
   ####
   In fs-mkdir.js, 85% are 'fs.mkdir 2 failed' and 15% are 'fs.mkdir 1 failed' with node-devel.
   In vanilla node, 100% are 'fs.mkdir2 failed'.

   rm -rf /tmp/t; mkdir /tmp/t; ./online/install.sh -i tests/module_races/fs/fs-mkdir.js  -d /tmp/t
   rm /tmp/o; for i in `seq 1 200`; do UV_THREADPOOL_SIZE=2 UV_SCHEDULER_MIN_DELAY=100000 UV_SCHEDULER_MAX_DELAY=100000 node-devel /tmp/t/fs-mkdir/fs-mkdir.js >> /tmp/o 2>&1; echo $i; done;
   grep -c 'fs.mkdir 2: failed' /tmp/o
     170
   grep -c 'fs.mkdir 1: failed' /tmp/o
     30

   rm /tmp/o; for i in `seq 1 200`; do UV_THREADPOOL_SIZE=2 node /tmp/t/fs-mkdir/fs-mkdir.js >> /tmp/o 2>&1; echo $i; done;
   grep -c 'fs.mkdir 1: failed' /tmp/o
     0
   grep -c 'fs.mkdir 2: failed' /tmp/o
     101


    ####
    # mkdirp (real application)
    ####
    node-devel shows a wider variety of behaviors than vanilla node.

    online/install.sh -i 19 -d /tmp/mkdir

    jamie@smellythingMint ~/Documents/research/noderacer2/nodejsrr/jamie $ rm /tmp/o; for i in `seq 1 100`; do UV_THREADPOOL_SIZE=2 UV_SCHEDULER_MIN_DELAY=100000 UV_SCHEDULER_MAX_DELAY=100000 node-devel /tmp/mkdirp/node-mkdirp-2/triggerRace.js >> /tmp/o 2>&1; echo $i; done 
    jamie@smellythingMint ~/Documents/research/noderacer2/nodejsrr/jamie $ grep APP /tmp/o | grep Successfully | grep -c UNIQUE_DIR_0
    66
    jamie@smellythingMint ~/Documents/research/noderacer2/nodejsrr/jamie $ grep APP /tmp/o | grep Successfully | grep -c UNIQUE_DIR_1
    57
    jamie@smellythingMint ~/Documents/research/noderacer2/nodejsrr/jamie $ grep APP /tmp/o | grep Successfully | wc -l
    123

    ^ Note that we see more than 100 successes; in some cases, there was no race and both mkdirs succeeded.
      In at least 25% of the cases, UNIQUE_DIR_1 was created.

    Compared to vanilla node, we see 

    jamie@smellythingMint ~/Documents/research/noderacer2/nodejsrr/jamie $ rm /tmp/o; for i in `seq 1 100`; do UV_THREADPOOL_SIZE=2 node /tmp/mkdirp/node-mkdirp-2/triggerRace.js >> /tmp/o 2>&1; echo $i; done 
    jamie@smellythingMint ~/Documents/research/noderacer2/nodejsrr/jamie $ grep APP /tmp/o | grep Successfully | grep -c UNIQUE_DIR_0
    91
    jamie@smellythingMint ~/Documents/research/noderacer2/nodejsrr/jamie $ grep APP /tmp/o | grep Successfully | grep -c UNIQUE_DIR_1
    9
    jamie@smellythingMint ~/Documents/research/noderacer2/nodejsrr/jamie $ grep APP /tmp/o | grep Successfully | wc -l
    100

    ^ Here we see one, or the other, but not both, succeed.
      In 9% of the cases, UNIQUE_DIR_1 was created.

6. Work on "degrees of freedom" idea. b4cd5425 has the first steps.

--------------------------------

Mon 26 Sep 2016

1. Improve scheduler arg processing in uv-common
2. Refactor scheduler_fuzzing_timer_thread_yield so that it's understandable
3. Work on the "degrees of freedom" scheduler
  > Next step is to test it

--------------------------------

Tue 27 Sep 2016

1. Add SCHEDULER_POINT_LOOPER_GETTING_DONE
2. Refactor to avoid duplicated code when ensuring mutex during CB execution
3. Test the "degrees of freedom" (TP_FREEDOM) scheduler
  > I ran 100 iterations on fs-mkdir.js and fs-rw.js, and the TP thread never had more than one item in the queue at a time.
    However, the looper thread consistently did have multiple items in the queue at a time.

    I think the fact that the TP thread had only one item at a time is an artifact of the fact that these tests have only 2 concurrent TP items at a time.
    So let's avoid "gaming the system" until we've tried a more complex test.

    New test: fs-manyIO.js
      This has an fs rw race with 10 fs writes in between the w and the r.
      This fills the TP work queue faster than the TP thread can empty it, so the TP_FREEDOM scheduler can perturb the schedule more significantly.

    Using the TP_FREEDOM scheduler, we observe w -> r 57% and r -> w 43%.
    Using the FUZZING_TIMER scheduler, we observe w -> r 97% and r -> w 3%.
    Using the vanilla node, we observe w -> r 100% and r-> w 0%.

    TP_FREEDOM
      $ rm /tmp/o; rm /tmp/err; for i in `seq 1 100`; do UV_THREADPOOL_SIZE=1 UV_SCHEDULER_TYPE=TP_FREEDOM UV_SCHEDULER_DEG_FREEDOM=5 node-devel /tmp/fs-manyIO/fs-manyIO/fs-manyIO.js >>/tmp/o 2>>/tmp/err; echo $i; done
      $ grep -c exiting /tmp/o
      43
      $ grep -c 'BEFOREAFTER' /tmp/o
      57

    FUZZING_TIMER (3 different parameters):

      MAX_DELAY 0.1 seconds. This test took approximately 2 minutes to run, at about 1 second per run. w -> r 97%, r -> w 3%.
        $ rm /tmp/o; rm /tmp/err; for i in `seq 1 100`; do UV_THREADPOOL_SIZE=5 UV_SCHEDULER_TYPE=FUZZING_TIMER UV_SCHEDULER_MIN_DELAY=1 UV_SCHEDULER_MAX_DELAY=100000 UV_SCHEDULER_DELAY_PERC=10 node-devel /tmp/fs-manyIO/fs-manyIO/fs-manyIO.js >>/tmp/o 2>>/tmp/err; echo $i; done
        $ grep -c 'BEFOREAFTER' /tmp/o
        97
        $ grep -c exiting /tmp/o
        3

      MAX_DELAY 0.01 seconds. This test ran more quickly. w -> r 99%, r -> w 1%
        $ rm /tmp/o; rm /tmp/err; for i in `seq 1 100`; do UV_THREADPOOL_SIZE=5 UV_SCHEDULER_TYPE=FUZZING_TIMER UV_SCHEDULER_MIN_DELAY=1 UV_SCHEDULER_MAX_DELAY=1000 UV_SCHEDULER_DELAY_PERC=10 node-devel /tmp/fs-manyIO/fs-manyIO/fs-manyIO.js >>/tmp/o 2>>/tmp/err; echo $i; done 
        $ grep -c 'BEFOREAFTER' /tmp/o
        99
        $ grep -c exiting /tmp/o
        1

      MAX_DELAY .001 seconds. This test ran more quickly. w -> r 100%, r -> w 0%
        $ rm /tmp/o; rm /tmp/err; for i in `seq 1 100`; do UV_THREADPOOL_SIZE=5 UV_SCHEDULER_TYPE=FUZZING_TIMER UV_SCHEDULER_MIN_DELAY=1 UV_SCHEDULER_MAX_DELAY=100 UV_SCHEDULER_DELAY_PERC=10 node-devel /tmp/fs-manyIO/fs-manyIO/fs-manyIO.js >>/tmp/o 2>>/tmp/err; echo $i; done 
        $ grep -c 'BEFOREAFTER' /tmp/o
        100
        $ grep -c exiting /tmp/o
        0

    VANILLA:
      $ rm /tmp/o; rm /tmp/err; for i in `seq 1 100`; do UV_THREADPOOL_SIZE=5 node /tmp/fs-manyIO/fs-manyIO/fs-manyIO.js >>/tmp/o 2>>/tmp/err; done 
      $ grep -c exiting /tmp/o
      0
      $ grep -c BEFOREAFTER /tmp/o
      100

    1. These results show that the TP_FREEDOM scheduler can trigger races that the vanilla node never sees.

    2. The worker thread handles 38 work items. On 24/38 (63%) occasions to take an item from the work queue, there is more than one item in the queue.
       The "high water mark" is 11 items, corresponding to an item from 11 of the 12 concurrent FS operations.
       This suggests that in an application where there's a non-trival amount of TP work, the TP_FREEDOM scheduler will likely have opportunities
       to perturb the execution.

         $ grep freedom_thread_yield /tmp/err | grep GETTING_WORK | wc -l
         38
         $ grep freedom_thread_yield /tmp/err | grep GETTING_WORK | grep -v '1/1' | wc -l
         24

       We could introduce a "Wait until TIMEOUT or looper thread is blocked" policy to maximize the size of the work queue, to increase the
       probability of having a choice. However, given these results, this tactic seems unnecessary for now.

4. Enhance the "degrees of freedom" scheduler to rein in the TP thread according to a few policies before letting it get work.
   The TP thread will now refrain from getting work until the scheduler lets it do so.
   The scheduler's decision is based on:
     - how long the TP thread has been waiting
     - how many items are in the queue
     - whether the looper thread appears to be blocked in epoll

--------------------------------

Wed 28 Sep 2016

1. Meeting with DY
2. "Vanilla" scheduler for comparative purposes (e.g. measuring system overhead)
3. Implement "epoll fuzzer". It has parameters UV_SCHEDULER_IOPOLL_DEG_FREEDOM, UV_SCHEDULER_IOPOLL_DEFER_PERC.

   To test this I set up the http_closed_system_{easy,medium,hard}.js tests.
   In these tests, a "surprising" ordering of client HTTP request arrivals can yield an uncaught ECONNREFUSED exception, crashing the application.

   $ rm -rf /tmp/http_closed_system_hard; mkdir /tmp/http_closed_system_hard; ./online/install.sh -i tests/no_races/complex/http_closed_system_hard.js  -d /tmp/http_closed_system_hard
   $ rm core; for i in `seq 1 100`; do UV_THREADPOOL_SIZE=1 UV_SCHEDULER_TYPE=TP_FREEDOM UV_SCHEDULER_TP_DEG_FREEDOM=5 UV_SCHEDULER_TP_MAX_DELAY=1000 UV_SCHEDULER_TP_EPOLL_THRESHOLD=1000 UV_SCHEDULER_IOPOLL_DEG_FREEDOM=-1 UV_SCHEDULER_IOPOLL_DEFER_PERC=0 node-devel /tmp/http_closed_system_hard/http_closed_system_hard/http_closed_system_hard.js >/tmp/o_$i 2>/tmp/err_$i; echo "iter $i: rc $?"; done

    This yields one of:
      - rc 0      Successful excution
      - rc 1      Uncaught exception ECONNREFUSED is thrown. The result of either epoll fuzzing interleaving or the threadpool delay. It doesn't happen without high-enough TP scheduler delays.
      - rc 139    Segfault in uv__read (map_hash). A bug in my logging code. This is unusual, and I'm not going to worry about it right now.

    20% of the time we get a bad order and fail with ECONNREFUSED. 
    $ grep -c ECONNREFUSED /tmp/err_* | grep -v ':0' | wc -l
    20

    On my laptop, we get

    $ grep ECONN /tmp/err_* | wc -l
    4

    , which difference we might want to investigate further.

    With "vanilla node", we see this race 0% of the time.

    $ rm core; for i in `seq 1 100`; do UV_THREADPOOL_SIZE=5 node /tmp/http_closed_system_hard/http_closed_system_hard/http_closed_system_hard.js >/tmp/o_$i 2>/tmp/err_$i; echo "iter $i: rc $?"; done
    $ grep -c ECONNREFUSED /tmp/err_* | grep -v ':0' | wc -l
    0

4. I reviewed libuv_src_notes to identify places where a single fd wraps up multiple events at the libuv architectural level (as opposed to at the Node.js level, like timers were):
    - TP done queue
    - FS events
    - async handles

5. TCP server: Does a single fd store up multiple pending connections (an fd with a "sub-queue", just like the "done queue" fd)? If so, could deferring and so on be problematic in terms of space exploration?

  A TCP server is an instance of a uv_tcp_t which "is a" uv_stream_t. The fd for a server-type stream is monitored in uv__server_io. 
  In uv__server_io, uv__accept is called repeatedly to accept incoming connections to the server's socket.
  For each accepted connection, the UV_CONNECTION_CB associated with the uv_stream_t is invoked so that the user has the option to call uv_accept to 
  complete the connection at the user level.
    (man 2 accept: "It extracts the first connection request on the queue of pending connections for the listening socket, sockfd, creates a new connected socket, and returns a new file descriptor referring to that socket.")

  This means that if there are $k multiple pending connections to a socket, the socket's fd will be ready in epoll, and $k UV_CONNECTION_CBs will be called.
  However, each connection will then get its own fd via uv_accept, and each fd will then be monitored individually by epoll.

  At the Node.js level, a UV_CONNECTION_CB corresponds to the emission of a 'listening' event: "Emitted when the server has been bound after calling server.listen." (https://nodejs.org/api/net.html#net_event_listening).
  So, the longer we defer a "server" fd, the more UV_CONNECTION_CBs might be invoked in a row when we finally invoke it in uv__io_poll.
  However, the 'data' event ("Emitted when data is received") will be emitted corresponding to the per-connection fd being invoked in uv__io_poll.

  So, our fuzzing scheme may result in a "surprising" number of consecutive UV_CONNECTION_CB invocations, but I don't anticipate those to be the source of bugs.

6. Begin work on a per-item "done fd" for the unified epoll queue (inputs + done queue).
   This development is being done on a separate branch in case it breaks things.

   So far, I've:
    - added a uv_async_t to uv__work (OK OK I added a 1KB buffer that I'm treaing as a uv_async_t, sue me)
    - changed worker to uv_async_send the uv_async_t in the work item rather than uv_async_send'ing the loop's wq
    - made a simple "handle the done CB for this work item" method for the uv_async_cb
   However, since async handles are implemented with a single fd, the result is that we now have *less* control over when done items get executed.
   Before, we could pick from the done queue in any order.
   Now, when we execute the "async wrapper fd" in uv__io_poll, we execute every pending done item, (I think) in the order in which the associated work was uv__work_submit'd.

   Next step: Re-implement uv_async_t less efficiently; monitor an fd per uv_async_t.
   Then we'll have each done item as a separate entry in the epoll events, and as a bonus we'll also address any other uv_async_t along the way.

--------------------------------

Thur 29 Sep 2016

Nada

--------------------------------

Fri 30 Sep 2016

1. The idea of "compressing" multiple fds under a single "wrapper" fd is multiplexing, though it might have other names too. 
   https://en.wikipedia.org/wiki/Multiplexing

2. I've de-multiplexed async handles. There is now one fd per async handle; each handle is monitored by the loop.
   Combined with 71da069d50, which gave each "work" item a uv_async_t, each "done" item from the threadpool is now monitored by the loop.
   This means that the TP_FREEDOM scheduler's UV_SCHEDULER_IOPOLL_DEFER_PERC and UV_SCHEDULER_IOPOLL_DEG_FREEDOM parameters influence the "done order"
   of TP items.

  Some experiments on fs-manyIO:
  
  1. Giving only IOPOLL_DEG_FREEDOM (no TP freedom), the race manifests itself 27% of the time, while 73% of the time it does not.

  $ rm -rf /tmp/fs-manyIO; mkdir /tmp/fs-manyIO; ./online/install.sh -i tests/module_races/fs/fs-manyIO.js -d /tmp/fs-manyIO
  $ for i in `seq 1 100`; do rm /tmp/o_$i /tmp/err_$i; UV_THREADPOOL_SIZE=1 UV_SCHEDULER_TYPE=TP_FREEDOM UV_SCHEDULER_TP_DEG_FREEDOM=1 UV_SCHEDULER_TP_MAX_DELAY=0 UV_SCHEDULER_TP_EPOLL_THRESHOLD=0 UV_SCHEDULER_IOPOLL_DEG_FREEDOM=10 UV_SCHEDULER_IOPOLL_DEFER_PERC=25 node-devel /tmp/fs-manyIO/fs-manyIO/fs-manyIO.js > /tmp/o_$i 2>/tmp/err_$i; echo "run $i rc $?"; done
  $ grep -c '<BEFORE>' /tmp/o_* | grep -v :0 | wc -l
    27
  $ grep -c '<BEFOREAFTER>' /tmp/o_* | grep -v :0 | wc -l
    73

  These parameters don't give particularly stable results, since other runs yield manifestation rates of 43% and 12%.

  $ for i in `seq 1 100`; do rm /tmp/o_$i /tmp/err_$i; UV_THREADPOOL_SIZE=1 UV_SCHEDULER_TYPE=TP_FREEDOM UV_SCHEDULER_TP_DEG_FREEDOM=1 UV_SCHEDULER_TP_MAX_DELAY=0 UV_SCHEDULER_TP_EPOLL_THRESHOLD=0 UV_SCHEDULER_IOPOLL_DEG_FREEDOM=10 UV_SCHEDULER_IOPOLL_DEFER_PERC=25 node-devel /tmp/fs-manyIO/fs-manyIO/fs-manyIO.js > /tmp/o_$i 2>/tmp/err_$i; echo "run $i rc $?"; done
  $ grep -c '<BEFORE>' /tmp/o_* | grep -v :0 | wc -l
  43
  $ grep -c '<BEFOREAFTER>' /tmp/o_* | grep -v :0 | wc -l
  57

  $ for i in `seq 1 100`; do rm /tmp/o_$i /tmp/err_$i; UV_THREADPOOL_SIZE=1 UV_SCHEDULER_TYPE=TP_FREEDOM UV_SCHEDULER_TP_DEG_FREEDOM=1 UV_SCHEDULER_TP_MAX_DELAY=0 UV_SCHEDULER_TP_EPOLL_THRESHOLD=0 UV_SCHEDULER_IOPOLL_DEG_FREEDOM=10 UV_SCHEDULER_IOPOLL_DEFER_PERC=25 node-devel /tmp/fs-manyIO/fs-manyIO/fs-manyIO.js > /tmp/o_$i 2>/tmp/err_$i; echo "run $i rc $?"; done
  $ grep -c '<BEFOREAFTER>' /tmp/o_* | grep -v :0 | wc -l
    88
  $ grep -c '<BEFORE>' /tmp/o_* | grep -v :0 | wc -l
    12

  The manifestations appear in "runs", so I anticipate that the variation is due to the RNG seed (which is based on the current time in seconds).
  I introduced a 1-second sleep between runs to ensure that invocations would get a different seed.
  Here, I get more reasonable-looking results (not runs of failures and successes, but rather failures sprinkled amongst the successes).
  In 2 separate trials of 200 iterations each, I observe manifestation rates of 24% and 19%.
  
  $ for i in `seq 1 200`; do rm /tmp/o_$i /tmp/err_$i; UV_THREADPOOL_SIZE=1 UV_SCHEDULER_TYPE=TP_FREEDOM UV_SCHEDULER_TP_DEG_FREEDOM=1 UV_SCHEDULER_TP_MAX_DELAY=0 UV_SCHEDULER_TP_EPOLL_THRESHOLD=0 UV_SCHEDULER_IOPOLL_DEG_FREEDOM=10 UV_SCHEDULER_IOPOLL_DEFER_PERC=25 node-devel /tmp/fs-manyIO/fs-manyIO/fs-manyIO.js > /tmp/o_$i 2>/tmp/err_$i; echo "run $i rc $?"; sleep 1; done
  $ grep -c '<BEFORE>' /tmp/o_* | grep -v :0 | wc -l
    48
  $ grep -c '<BEFOREAFTER>' /tmp/o_* | grep -v :0 | wc -l
    152

  $ for i in `seq 1 200`; do rm /tmp/o_$i /tmp/err_$i; UV_THREADPOOL_SIZE=1 UV_SCHEDULER_TYPE=TP_FREEDOM UV_SCHEDULER_TP_DEG_FREEDOM=1 UV_SCHEDULER_TP_MAX_DELAY=0 UV_SCHEDULER_TP_EPOLL_THRESHOLD=0 UV_SCHEDULER_IOPOLL_DEG_FREEDOM=10 UV_SCHEDULER_IOPOLL_DEFER_PERC=25 node-devel /tmp/fs-manyIO/fs-manyIO/fs-manyIO.js > /tmp/o_$i 2>/tmp/err_$i; echo "run $i rc $?"; sleep 1; done
  $ grep -c '<BEFORE>' /tmp/o_* | grep -v :0 | wc -l
    38
  $ grep -c '<BEFOREAFTER>' /tmp/o_* | grep -v :0 | wc -l
    162

  -----

  2. Giving only TP freedom and eliminating IOPOLL freedom, the effectiveness decreases to 11% race manifestation, 89% failure. 

  $ for i in `seq 1 100`; do rm /tmp/o_$i /tmp/err_$i; UV_THREADPOOL_SIZE=1 UV_SCHEDULER_TYPE=TP_FREEDOM UV_SCHEDULER_TP_DEG_FREEDOM=5 UV_SCHEDULER_TP_MAX_DELAY=1000 UV_SCHEDULER_TP_EPOLL_THRESHOLD=1000 UV_SCHEDULER_IOPOLL_DEG_FREEDOM=1 UV_SCHEDULER_IOPOLL_DEFER_PERC=0 node-devel /tmp/fs-manyIO/fs-manyIO/fs-manyIO.js > /tmp/o_$i 2>/tmp/err_$i; echo "run $i rc $?"; done
  $ grep -c '<BEFORE>' /tmp/o_* | grep -v :0 | wc -l
  11
  $ grep -c '<BEFOREAFTER>' /tmp/o_* | grep -v :0 | wc -l
  89

  -----

  3. Like 2, but further reducing the delays the TP allows before proceeding with work, the fuzzer is completely ineffective: 0% race manifestation.

  $ for i in `seq 1 100`; do rm /tmp/o_$i /tmp/err_$i; UV_THREADPOOL_SIZE=1 UV_SCHEDULER_TYPE=TP_FREEDOM UV_SCHEDULER_TP_DEG_FREEDOM=5 UV_SCHEDULER_TP_MAX_DELAY=100 UV_SCHEDULER_TP_EPOLL_THRESHOLD=100 UV_SCHEDULER_IOPOLL_DEG_FREEDOM=1 UV_SCHEDULER_IOPOLL_DEFER_PERC=0 node-devel /tmp/fs-manyIO/fs-manyIO/fs-manyIO.js > /tmp/o_$i 2>/tmp/err_$i; echo "run $i rc $?"; done
  $ grep -c '<BEFOREAFTER>' /tmp/o_* | grep -v :0 | wc -l
    100

  Vanilla node still finds the race 0% of the time.

  $ rm -rf /tmp/fs-manyIO; mkdir /tmp/fs-manyIO; ./online/install.sh -i tests/module_races/fs/fs-manyIO.js -d /tmp/fs-manyIO
  $ for i in `seq 1 100`; do rm /tmp/o_$i /tmp/err_$i; UV_THREADPOOL_SIZE=10 node /tmp/fs-manyIO/fs-manyIO/fs-manyIO.js > /tmp/o_$i 2>/tmp/err_$i; echo "run $i rc $?"; done
  $ grep -c '<BEFOREAFTER>' /tmp/o_* | grep -v :0 | wc -l
    100

  -----

  4. Combining the degrees of freedom for both TP and IOPOLL and using the timing technique from 1 yields a manifestation rate of 26%. This is a modest boost (4%) compared to 1, unclear if it's statistically significant.

  $ rm /tmp/o_* /tmp/err_*; for i in `seq 1 200`; do UV_THREADPOOL_SIZE=1 UV_SCHEDULER_TYPE=TP_FREEDOM UV_SCHEDULER_TP_DEG_FREEDOM=5 UV_SCHEDULER_TP_MAX_DELAY=1000 UV_SCHEDULER_TP_EPOLL_THRESHOLD=1000 UV_SCHEDULER_IOPOLL_DEG_FREEDOM=10 UV_SCHEDULER_IOPOLL_DEFER_PERC=25 node-devel /tmp/fs-manyIO/fs-manyIO/fs-manyIO.js > /tmp/o_$i 2>/tmp/err_$i; echo "run $i rc $?"; sleep 1; done
  $ grep -c '<BEFORE>' /tmp/o_* | grep -v :0 | wc -l
  53
  $ grep -c '<BEFOREAFTER>' /tmp/o_* | grep -v :0 | wc -l
  146

--------------------------------

Sat 01 Oct 2016

1. Finish up async de-multiplexing and make a commit

2. Potential other sources of multiplexing (uv-unix.h: members of uv_loop_s):
    uv__io_t signal_io_watcher;
    uv_signal_t child_watcher;

3. Begin working on testing npm modules.

  ~/Documents/research/nodefuzzer_modules_to_test/node_modules

  Issues so far:
    - "npm install" doesn't seem to be installing dependencies correctly, so "npm test" often fails with missing dependencies
      > e.g. mocha, babel-register
      > This issue was because we weren't using the '--dev' flag with npm install
      > See notes in node_test_suite_journal.txt

--------------------------------

Sun 02 Oct 2016

1. Continue working on node test suite testing. Currently working on mkdirp.

--------------------------------

Mon 03 Oct 2016

1. Explore the mkdirp race more carefully.
   Conclusion: Unless a module has randomized or "system test" type tests, its tests are unlikely to be helpful to us.
   The mkdirp triggerRace.js hits the race almost every time even on vanilla node, because it's targeted at the race.
   A developer presumably wouldn't check in a test that couldn't reliably trigger the race, so using TP_FREEDOM
   might actually *decrease* the incidence of the race because we introduce randomness.

   Instead, I opted to write an application modeled after the bug report.
   In this case the bug doesn't manifest itself regularly under vanilla, but TP_FREEDOM can cause it to appear.
   Unfortunately, bug reports typically only include "minimal" recreate scenarios, due to the difficulty of reproducing
   non-deterministic bugs on vanilla node.
   
   This means that demonstrating the usefulness of our system could require developing a non-trivial supply of test drivers.
   This might be unpleasantly labor-intensive, and will diminish how readily we can show the impact of our approach.

   HOWEVER from mkdirp I learned how to install modules and run their tests, so perhaps I'll get a few more complex ones and see if anything turns up
   before throwing in the towel.

2. I tried to install and run the tests of a bunch of file system-themed modules.
   See npm_module_install_and_test_results, but basically: 14 file system-themed modules, 1 successful installation+test suite, that suite hangs on my version of node.
   Some development installations failed with ELIFECYCLE, presumably because my "vanilla" version of node is a bit old.

   The module with passing tests had 6K downloads in the past month.
   Most of the modules without tests had minimal downloads in the past month (high of 150). 
   Of the modules with "npm test" failing with syntax errors, one of the modules without tests had 250 downloads this month, and the other had 144K downloads this month.

   Summary:
      - The most popular module's "npm test" failed.
      - 1/14 had plain installation fail (libuv-fs)
      - 3/14 had ELIFECYCLE on devel installation (file-system-css, file-system-cache, file-system-lint)
      - 6/14 had no tests (cordova-zip-file-system, extended-fs, fs-io, sync-fs, promise-fs, fs-secure)
      - 3/14 had "npm test" fail with syntax or dependency(?) errors (fs-rpc, file-system-store, fs-promise)
      - 1/14 had devel installation and "npm test" succeed (file-system)

  > Need to look into this hang, I'm guessing it's the same one as on 'npm test' for mkdirp (which hangs at the end of the "tap" run).

--------------------------------

Tue 04 Oct 2016

1. Improve fuzz_benchmarks naming conventions.

2. Add a fancier changeNode into noderacer2/bin.

3. Add a test_npm_module script into noderacer2/bin.
   This script goes through the specified modules and tries to:
     - install
     - devel install
     - npm test
   each of them. It prints its findings, including a result code and the failing or successful command + output.

   I ran it using node-devel compiled on the master (i.e. original clone point) branch.
   This version of node has the same "version string" (node -v) as our node-devel's from MultiSched_NoDoneQueue,
   so any dependency issues will be the same as those when using our version of node.

   $ readlink /usr/bin/node
     /home/jamie3/Desktop/noderacer2/node_versions/node-devel.master
   $ cat /tmp/modules
     file-system-css
     file-system-cache
     file-system-store
     file-system
     file-system-lint
     cordova-zip-file-system
     extended-fs
     fs-rpc
     fs-io
     sync-fs
     fs-promise
     promise-fs
     libuv-fs
     fs-secure
     forever
   $ time ./test_npm_module --moduleFile /tmp/modules >/tmp/out
     real 3m25.596s
     user 2m45.504s
     sys  0m12.350s
   $ for res in "INSTALL FAILED" "DEVEL INSTALL FAILED" "TEST FAILED" "SUCCESS"; do echo $res; grep -c "result $res" /tmp/out; done
     INSTALL FAILED
     1
     DEVEL INSTALL FAILED
     4
     TEST FAILED
     5
     SUCCESS
     5

  Next step is to compare using our version of node, but to do that we need to fix the sometime "npm test" hang.

4. Discussed status with DY.
   Decision was that if we can demonstrate the feasibility of our fuzzing system on Arun's races, we can submit to EuroSys.
   Therefore, we'll focus on understanding and recreating Arun's races, with particular focus on showing that "vanilla node" is inadequate
   for all but extremely targeted testing.

   My notesheet from 10/4 has information about the node-logger, agent-keepalive, socket.io-client, and pep-steelskin bugs.

--------------------------------

Wed 05 Oct 2016

1. Remove LCBN parentage tracking. We need tests to run more than we need pretty graphs.
   See 1db522d949f for details.

2. Explore node-logger-1 race, mongoose race. Notes in the corresponding README.fuzz in each fuzz_benchmarks dir.

  Summary: fuzz tester does a good job.

3. Begin work on agent-keepalive. This one is trickier because it depends on a user operation "just happening" to interact with a timer whose length is somewhat hidden.
   To support my attempts on agent-keepalive, I've added timer fuzzing. This has proved more effective and relies less on "lucky timings".

   $ UV_THREADPOOL_SIZE=1 UV_SCHEDULER_TYPE=TP_FREEDOM UV_SCHEDULER_TP_DEG_FREEDOM=5 UV_SCHEDULER_TP_MAX_DELAY=1000 UV_SCHEDULER_TP_EPOLL_THRESHOLD=1000 UV_SCHEDULER_IOPOLL_DEG_FREEDOM=-1 UV_SCHEDULER_IOPOLL_DEFE
    > This hits ECONNRESET with more regularity than without timer fuzzing

   HOWEVER: I think it's still a bit awkward because core.c and linux-core.c make use of the "nearest timer" to determine how long to epoll.
            If we're fuzzing timers, we don't actually want to do that.

--------------------------------

Thur 06 Oct 2016

1. Finish agent-keepalive testing. Misc. improvements to timer fuzzing scheme.

2. Began work on socket.io-client

  This command works using "vanilla node" and "devel node - master" when test/disconnect.js has only 1 of the 4 tests uncommented.
  Not sure what the story is there.
    (16:49:39) jamie@suwarna7-Lenovo-K450e ~/Desktop/noderacer2/fuzz_benchmarks/socket.io-1862_FOO $ DISCONNECT_DELAY1=100 DISCONNECT_DELAY2=200 ./node_modules/.bin/mocha --reporter=spec test/disconnect.js 
    (14:29:52) jamie@suwarna7-Lenovo-K450e ~/Desktop/noderacer2/fuzz_benchmarks/socket.io-1862_FOO $ UV_SCHEDULER_TYPE=VANILLA DISCONNECT_DELAY1=0 DISCONNECT_DELAY2=0 ./node_modules/.bin/mocha --reporter=spec test/disconnect.js 2>/tmp/err

--------------------------------

Fri 07 Oct 2016

1. Fix racy access to struct uv__work in threadpool.c reported by:
    (23:08:37) jamie@suwarna7-Lenovo-K450e ~/Desktop/noderacer2/fuzz_benchmarks/socket.io-1862_FOO/fuzz_test $ rm core; UV_THREADPOOL_SIZE=1 UV_SCHEDULER_TYPE=VANILLA DISCONNECT_DELAY1=100 DISCONNECT_DELAY2=200 valgrind node-devel triggerRace.js
   
   This allows us to test socket.io-1862's triggerRaces.js using node, though the mocha run still fails weirdly.
   Nevertheless, progress!
 
 2. Fix racy access to uv_handle_t after calling its UV_CLOSE_CB (which might delete the handle).
   Thanks, valgrind!
 
 3. Fix mocha error that caused it to skip tests. I was double-calling idle/check/prepare CBs.

 4. Mocha tests have a default suite-wide timeout of 2000 ms. This can be overridden using the '--timeout' flag on the mocha invocation.
    However, the timer fuzzer won't work well with this, since it will accelerate the "test timeout" timer too.

    Solutions:  1. "blacklist" very long timers (Eh...).
               *2. Call "this.timeout(0)" at the beginning of the mocha suite, and disable any per-test timeouts (which override the suite-wide value).
                   This means that any tests that hang will just hang forever.
                   You can sort-of address this with 'timeout ntests*2 mocha ...';
                    this is the maximum time in seconds that the entire test suite could take.
                    However, this timeout is much longer than an individual test case would take to timeout, so for a large test suite this could be expensive.
                    This concern is only relevant if a failing test case can legitimately manifest as an indefinite hang, though.

  Run socket.io triggerRace.js:
    (23:09:17) jamie@suwarna7-Lenovo-K450e ~/Desktop/noderacer2/fuzz_benchmarks/socket.io-1862_FOO/fuzz_test $ rm core; UV_THREADPOOL_SIZE=1 UV_SCHEDULER_TYPE=VANILLA DISCONNECT_DELAY1=100 DISCONNECT_DELAY2=200 node-devel-debug triggerRace.js 2>/tmp/err
    (23:10:01) jamie@suwarna7-Lenovo-K450e ~/Desktop/noderacer2/fuzz_benchmarks/socket.io-1862_FOO/fuzz_test $ UV_SCHEDULER_TYPE=VANILLA DISCONNECT_DELAY1=100 DISCONNECT_DELAY2=200 node triggerRace.js

  Run triggerRace using "vanilla node":
    (23:10:16) jamie@suwarna7-Lenovo-K450e ~/Desktop/noderacer2/fuzz_benchmarks/socket.io-1862_FOO/fuzz_test $ DISCONNECT_DELAY1=100 DISCONNECT_DELAY2=200 node triggerRace.js 

5. Complete socket-io test case testing.

6. Perform socket-io test *suite* testing. See README.fuzz for details.

7. Begin work on nes

--------------------------------

Sat 08 Oct 2016

1. I've discovered that we can legally shuffle timers. https://nodejs.org/api/timers.html#timers_settimeout_callback_delay_arg 
   Add notion of "timer degrees of freedom". 
  
2. Work on nes testing.
    > Developed "randomer" test case, maybe too contrived?

--------------------------------

Mon 10 Oct 2016

1. Don't shuffle timers when delaying them.
2. Finish nes testing.
3. Add "fuzz-less" experiment for node-logger.
4. Remove agent-keepalive from the list of successes. Changes in scheduler mean that I no longer have a working test case.
   However, the initial experiments were good on another flavor of the fuzzer; can we still report the results?

--------------------------------

Tue 11 Oct 2016

1. Verify that "play-it-straight devel node" doesn't produce bugs as easily as devel node does.
   This lets us be confident that our fuzzer is actually useful.

2. Explore and patch the most blatant race in the socket.io test suite.

--------------------------------

Wed 12 Oct 2016

1. Working on paper writing.

2. To compute LoC, try "git diff --stat master MultiSched -- '*.c' '*.h'"
    (need to filter out some unused things for fairness, e.g. the changes in src/win)
      -> I should just revert these changes...

--------------------------------

Thur 13 Oct 2016

1. More paper writing.
2. Added uv__run_closing_handles fuzzing for agentkeepalive. Details in the associated README.fuzz.

--------------------------------

Fri 14 Oct 2016

1. Bug recreates on kue, pep, ghost, etherpad, ...?

-----


TODO
  - Submit pull request for socket.io test suite bug
  - Attempt fuzz testing on: kue. pep. ghost/etherpad?
  - Try core module test cases, since they don't need a working test suite.
  - "Timer fuzzing": the "delay" might only be relevant if we delay it beyond 'k' other CBs.
  - Extend test_npm_module with a "fast" mode that simply checks if the npm module has tests. 

      package.json:
      { "scripts" :
        { ...
          "test" : "make test"
          ...
        }
      }

      Use a perl JSON parser, see http://search.cpan.org/~bkb/JSON-Parse-0.42/lib/JSON/Parse.pod
  - How does nextTick look at the libuv level?
  - Fix "npm test" hang: mocha (file-system), tap (mkdirp)
    > This is needful if we want to test a random selection of modules in search of bugs, but the state of the test suites thus far suggests that
      if there are *any* tests, they're focused on unit tests rather than "application-type" (system) testing.
      So not confident that this project is worthwhile.
    > Try with various versions of node to see when the hang was introduced, starting with 'git checkout master' so we actually know whether it ever worked.
      < It did in fact work at some point. I compiled on 'master' branch and it ran OK on the file-system tests.
    > Could use git-bisect, if I knew that every commit compiled...which I don't
      - That's a good reason to make good commits, I suppose.

-----------

STEPS:
- An interesting comparison: "random testing" inspired by the empirical study. At the JS level, insert rand_sleep into any asynchronous method. See how much variation in the schedule arises.

  Feb. 18: 
    X inheritance coloring, favoring logical parentage - 1/2 day
    X explore the library used in Arun's example that offers X.thenY.thenZ semantics. I believe it's called promise.js. See whether it makes a "big blob" or uses a baton approach. Batons mean that we can find Arun's race condition; big blob means that it's truly a "threadpool race" and will only manifest if the two blobs are run concurrently. - 1/2 day
    X visualization of execution order, including overlapping CBs - 1/2 day
    o initial definition(s) and extraction(s) of schedule - 1 day
    o measure variations in schedule(s) for simple and more complex apps - 1 day
    o explore identification of "fat functions" a la word-finder. This is part of the analysis stage. 1/2 day

  Feb. 25:
    - Add a scheduled execution mode (i.e. follow the provided schedule as closely as possible). 3 days.
        May want per-client CB tree visualization?
        The tricky part is that the first step off of the original schedule might radically change the rest of the schedule -- e.g. httpserver_batch.js. Go back to record mode? Potentially too expensive.
    - Try naive schedule exploration of simple apps. 2 days.
    - This is probably too aggressive to get CORRECT in one week, but we'll see. I won't propose schedules beyond that because I'm not confident about the size of this milestone.
  
  Mar. 3:
    - ?
